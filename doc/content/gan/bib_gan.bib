@online{arjovskyWassersteinGAN2017,
  title = {Wasserstein {{GAN}}},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  date = {2017-12-06},
  eprint = {1701.07875},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1701.07875},
  url = {http://arxiv.org/abs/1701.07875},
  urldate = {2023-01-04},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {E\:\\data\\zotero\\storage\\XGJGG4PB\\Arjovsky 等 - 2017 - Wasserstein GAN.pdf;E\:\\data\\zotero\\storage\\NLZBVAYZ\\1701.html}
}

@online{berthelotBEGANBoundaryEquilibrium2017,
  title = {{{BEGAN}}: {{Boundary Equilibrium Generative Adversarial Networks}}},
  shorttitle = {{{BEGAN}}},
  author = {Berthelot, David and Schumm, Thomas and Metz, Luke},
  date = {2017-05-31},
  eprint = {1703.10717},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1703.10717},
  url = {http://arxiv.org/abs/1703.10717},
  urldate = {2023-01-04},
  abstract = {We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {E\:\\data\\zotero\\storage\\4LBUNGXI\\Berthelot 等 - 2017 - BEGAN Boundary Equilibrium Generative Adversarial.pdf;E\:\\data\\zotero\\storage\\KVPZE7SB\\1703.html}
}

@online{chanEfficientGeometryaware3D2022,
  title = {Efficient {{Geometry-aware 3D Generative Adversarial Networks}}},
  author = {Chan, Eric R. and Lin, Connor Z. and Chan, Matthew A. and Nagano, Koki and Pan, Boxiao and De Mello, Shalini and Gallo, Orazio and Guibas, Leonidas and Tremblay, Jonathan and Khamis, Sameh and Karras, Tero and Wetzstein, Gordon},
  date = {2022-04-27},
  eprint = {2112.07945},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.07945},
  url = {http://arxiv.org/abs/2112.07945},
  urldate = {2023-04-17},
  abstract = {Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute-intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit-implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {E\:\\data\\zotero\\storage\\8PAV8FGJ\\Chan 等 - 2022 - Efficient Geometry-aware 3D Generative Adversarial.pdf;E\:\\data\\zotero\\storage\\27YR3YQ6\\2112.html}
}

@online{chanPiGANPeriodicImplicit2021,
  title = {Pi-{{GAN}}: {{Periodic Implicit Generative Adversarial Networks}} for {{3D-Aware Image Synthesis}}},
  shorttitle = {Pi-{{GAN}}},
  author = {Chan, Eric R. and Monteiro, Marco and Kellnhofer, Petr and Wu, Jiajun and Wetzstein, Gordon},
  date = {2021-04-05},
  eprint = {2012.00926},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.00926},
  url = {http://arxiv.org/abs/2012.00926},
  urldate = {2022-12-28},
  abstract = {We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks (\$\textbackslash pi\$-GAN or pi-GAN), for high-quality 3D-aware image synthesis. \$\textbackslash pi\$-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent 3D representations with fine detail. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {E\:\\data\\zotero\\storage\\V3ZTD3VM\\Chan 等 - 2021 - pi-GAN Periodic Implicit Generative Adversarial N.pdf;E\:\\data\\zotero\\storage\\VTFGL7WU\\2012.html}
}

@online{chenInfoGANInterpretableRepresentation2016,
  title = {{{InfoGAN}}: {{Interpretable Representation Learning}} by {{Information Maximizing Generative Adversarial Nets}}},
  shorttitle = {{{InfoGAN}}},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  date = {2016-06-11},
  eprint = {1606.03657},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1606.03657},
  url = {http://arxiv.org/abs/1606.03657},
  urldate = {2023-01-04},
  abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {E\:\\data\\zotero\\storage\\K4L9KMBW\\Chen 等 - 2016 - InfoGAN Interpretable Representation Learning by .pdf;E\:\\data\\zotero\\storage\\NPPHL6FP\\1606.html}
}

@online{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-06-10},
  eprint = {1406.2661},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1406.2661},
  url = {http://arxiv.org/abs/1406.2661},
  urldate = {2023-01-04},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {E\:\\data\\zotero\\storage\\IAQEL3L3\\Goodfellow 等 - 2014 - Generative Adversarial Networks.pdf;E\:\\data\\zotero\\storage\\YPTDMHXJ\\1406.html}
}

@online{gulrajaniImprovedTrainingWasserstein2017,
  title = {Improved {{Training}} of {{Wasserstein GANs}}},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
  date = {2017-12-25},
  eprint = {1704.00028},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1704.00028},
  url = {http://arxiv.org/abs/1704.00028},
  urldate = {2023-01-04},
  abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {E\:\\data\\zotero\\storage\\DQTL8SJ9\\Gulrajani 等 - 2017 - Improved Training of Wasserstein GANs.pdf;E\:\\data\\zotero\\storage\\98AF2ADA\\1704.html}
}

@online{isolaImagetoImageTranslationConditional2018,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  date = {2018-11-26},
  eprint = {1611.07004},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.07004},
  urldate = {2023-02-02},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D:\data\papers\image-to-image-translation-with-conditional-adversarial-networks.pdf}
}

@online{kangScalingGANsTexttoImage2023,
  title = {Scaling up {{GANs}} for {{Text-to-Image Synthesis}}},
  author = {Kang, Minguk and Zhu, Jun-Yan and Zhang, Richard and Park, Jaesik and Shechtman, Eli and Paris, Sylvain and Park, Taesung},
  date = {2023-03-09},
  eprint = {2303.05511},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.05511},
  url = {http://arxiv.org/abs/2303.05511},
  urldate = {2023-04-06},
  abstract = {The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination. From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models. GANs used to be the de facto choice, with techniques like StyleGAN. With DALL-E 2, auto-regressive and diffusion models became the new standard for large-scale generative models overnight. This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? We find that na\textbackslash "Ively increasing the capacity of the StyleGAN architecture quickly becomes unstable. We introduce GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, it can synthesize high-resolution images, for example, 16-megapixel pixels in 3.66 seconds. Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {E\:\\data\\zotero\\storage\\EJLK4UBU\\Kang 等 - 2023 - Scaling up GANs for Text-to-Image Synthesis.pdf;E\:\\data\\zotero\\storage\\3XDCCRWE\\2303.html}
}

@article{kimUnsupervisedDeepLearning2021,
  title = {Unsupervised Deep Learning for Super-Resolution Reconstruction of Turbulence},
  author = {Kim, Hyojin and Kim, Junhyuk and Won, Sungjin and Lee, Changghoon},
  date = {2021-03-10},
  journaltitle = {Journal of Fluid Mechanics},
  shortjournal = {J. Fluid Mech.},
  volume = {910},
  eprint = {2007.15324},
  eprinttype = {arxiv},
  eprintclass = {physics},
  pages = {A29},
  issn = {0022-1120, 1469-7645},
  doi = {10.1017/jfm.2020.1028},
  url = {http://arxiv.org/abs/2007.15324},
  urldate = {2023-04-05},
  abstract = {Recent attempts to use deep learning for super-resolution reconstruction of turbulent flows have used supervised learning, which requires paired data for training. This limitation hinders more practical applications of super-resolution reconstruction. Therefore, we present an unsupervised learning model that adopts a cycle-consistent generative adversarial network that can be trained with unpaired turbulence data for super-resolution reconstruction. Our model is validated using three examples: (i) recovering the original flow field from filtered data using direct numerical simulation (DNS) of homogeneous isotropic turbulence; (ii) reconstructing full-resolution fields using partially measured data from the DNS of turbulent channel flows; and (iii) generating a DNS-resolution flow field from large eddy simulation (LES) data for turbulent channel flows. In examples (i) and (ii), for which paired data are available for supervised learning, our unsupervised model demonstrates qualitatively and quantitatively similar performance as that of the best supervised-learning model. More importantly, in example (iii), where supervised learning is impossible, our model successfully reconstructs the high-resolution flow field of statistical DNS quality from the LES data. This demonstrates that unsupervised learning of turbulence data is indeed possible, opening a new door for the wide application of super-resolution reconstruction of turbulent fields.},
  keywords = {Physics - Fluid Dynamics},
  file = {E\:\\data\\zotero\\storage\\W26ZBDWJ\\Kim et al. - 2021 - Unsupervised deep learning for super-resolution re.pdf;E\:\\data\\zotero\\storage\\HPQ83GQN\\2007.html}
}

@online{kodaliConvergenceStabilityGANs2017,
  title = {On {{Convergence}} and {{Stability}} of {{GANs}}},
  author = {Kodali, Naveen and Abernethy, Jacob and Hays, James and Kira, Zsolt},
  date = {2017-12-10},
  eprint = {1705.07215},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1705.07215},
  url = {http://arxiv.org/abs/1705.07215},
  urldate = {2023-01-04},
  abstract = {We propose studying GAN training dynamics as regret minimization, which is in contrast to the popular view that there is consistent minimization of a divergence between real and generated distributions. We analyze the convergence of GAN training from this new point of view to understand why mode collapse happens. We hypothesize the existence of undesirable local equilibria in this non-convex game to be responsible for mode collapse. We observe that these local equilibria often exhibit sharp gradients of the discriminator function around some real data points. We demonstrate that these degenerate local equilibria can be avoided with a gradient penalty scheme called DRAGAN. We show that DRAGAN enables faster training, achieves improved stability with fewer mode collapses, and leads to generator networks with better modeling performance across a variety of architectures and objective functions.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {E\:\\data\\zotero\\storage\\RVDP98H9\\Kodali 等 - 2017 - On Convergence and Stability of GANs.pdf;E\:\\data\\zotero\\storage\\3KZYT8DQ\\1705.html}
}

@online{leeStyLandGANStyleGANBased2022,
  title = {{{StyLandGAN}}: {{A StyleGAN}} Based {{Landscape Image Synthesis}} Using {{Depth-map}}},
  shorttitle = {{{StyLandGAN}}},
  author = {Lee, Gunhee and Yim, Jonghwa and Kim, Chanran and Kim, Minjae},
  date = {2022-05-13},
  eprint = {2205.06611},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.06611},
  urldate = {2022-12-26},
  abstract = {Despite recent success in conditional image synthesis, prevalent input conditions such as semantics and edges are not clear enough to express ‘Linear (Ridges)’ and ‘Planar (Scale)’ representations. To address this problem, we propose a novel framework StyLandGAN, which synthesizes desired landscape images using a depth map which has higher expressive power. Our StyleLandGAN is extended from the unconditional generation model to accept input conditions. We also propose a ’2-phase inference’ pipeline which generates diverse depth maps and shifts local parts so that it can easily reflect user’s intend. As a comparison, we modified the existing semantic image synthesis models to accept a depth map as well. Experimental results show that our method is superior to existing methods in quality, diversity, and depth-accuracy.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {E:\data\zotero\storage\825VAK35\Lee et al. - 2022 - StyLandGAN A StyleGAN based Landscape Image Synth.pdf}
}

@online{maoLeastSquaresGenerative2017,
  title = {Least {{Squares Generative Adversarial Networks}}},
  author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
  date = {2017-04-05},
  eprint = {1611.04076},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1611.04076},
  url = {http://arxiv.org/abs/1611.04076},
  urldate = {2023-01-04},
  abstract = {Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson \$\textbackslash chi\^2\$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {E\:\\data\\zotero\\storage\\VWSDDXC8\\Mao 等 - 2017 - Least Squares Generative Adversarial Networks.pdf;E\:\\data\\zotero\\storage\\HPM83ARU\\1611.html}
}

@online{mirzaConditionalGenerativeAdversarial2014,
  title = {Conditional {{Generative Adversarial Nets}}},
  author = {Mirza, Mehdi and Osindero, Simon},
  date = {2014-11-06},
  eprint = {1411.1784},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1411.1784},
  url = {http://arxiv.org/abs/1411.1784},
  urldate = {2023-01-04},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {E\:\\data\\zotero\\storage\\CZHX5UI4\\Mirza 和 Osindero - 2014 - Conditional Generative Adversarial Nets.pdf;E\:\\data\\zotero\\storage\\SR7J9NXU\\1411.html}
}

@online{odenaConditionalImageSynthesis2017,
  title = {Conditional {{Image Synthesis With Auxiliary Classifier GANs}}},
  author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
  date = {2017-07-20},
  eprint = {1610.09585},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1610.09585},
  url = {http://arxiv.org/abs/1610.09585},
  urldate = {2023-01-04},
  abstract = {Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7\% of the classes have samples exhibiting diversity comparable to real ImageNet data.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  file = {E\:\\data\\zotero\\storage\\C7BUMA8E\\Odena 等 - 2017 - Conditional Image Synthesis With Auxiliary Classif.pdf;E\:\\data\\zotero\\storage\\N2I778VV\\1610.html}
}

@online{or-elStyleSDFHighResolution3DConsistent2022,
  title = {{{StyleSDF}}: {{High-Resolution 3D-Consistent Image}} and {{Geometry Generation}}},
  shorttitle = {{{StyleSDF}}},
  author = {Or-El, Roy and Luo, Xuan and Shan, Mengyi and Shechtman, Eli and Park, Jeong Joon and Kemelmacher-Shlizerman, Ira},
  date = {2022-03-29},
  eprint = {2112.11427},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.11427},
  url = {http://arxiv.org/abs/2112.11427},
  urldate = {2023-04-17},
  abstract = {We introduce a high resolution, 3D-consistent image and shape generation technique which we call StyleSDF. Our method is trained on single-view RGB data only, and stands on the shoulders of StyleGAN2 for image generation, while solving two main challenges in 3D-aware GANs: 1) high-resolution, view-consistent generation of the RGB images, and 2) detailed 3D shape. We achieve this by merging a SDF-based 3D representation with a style-based 2D generator. Our 3D implicit network renders low-resolution feature maps, from which the style-based network generates view-consistent, 1024x1024 images. Notably, our SDF-based 3D modeling defines detailed 3D surfaces, leading to consistent volume rendering. Our method shows higher quality results compared to state of the art in terms of visual and geometric quality.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {E\:\\data\\zotero\\storage\\XCYNWHRB\\Or-El 等 - 2022 - StyleSDF High-Resolution 3D-Consistent Image and .pdf;E\:\\data\\zotero\\storage\\ULKVRTBY\\2112.html}
}

@online{parkSemanticImageSynthesis2019,
  title = {Semantic {{Image Synthesis}} with {{Spatially-Adaptive Normalization}}},
  author = {Park, Taesung and Liu, Ming-Yu and Wang, Ting-Chun and Zhu, Jun-Yan},
  date = {2019-11-05},
  eprint = {1903.07291},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1903.07291},
  url = {http://arxiv.org/abs/1903.07291},
  urldate = {2023-01-10},
  abstract = {We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at https://github.com/NVlabs/SPADE .},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,I.3.3,I.5,I.5.4},
  file = {E\:\\data\\zotero\\storage\\25W2DAQH\\Park 等 - 2019 - Semantic Image Synthesis with Spatially-Adaptive N.pdf;E\:\\data\\zotero\\storage\\GHI3ENF5\\1903.html}
}

@online{zhaoEnergybasedGenerativeAdversarial2017,
  title = {Energy-Based {{Generative Adversarial Network}}},
  author = {Zhao, Junbo and Mathieu, Michael and LeCun, Yann},
  date = {2017-03-06},
  eprint = {1609.03126},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1609.03126},
  url = {http://arxiv.org/abs/1609.03126},
  urldate = {2023-01-04},
  abstract = {We introduce the "Energy-based Generative Adversarial Network" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {E\:\\data\\zotero\\storage\\VUTAF7RY\\Zhao 等 - 2017 - Energy-based Generative Adversarial Network.pdf;E\:\\data\\zotero\\storage\\TYU2TYN6\\1609.html}
}

@online{zhuUnpairedImagetoImageTranslation2020,
  title = {Unpaired {{Image-to-Image Translation}} Using {{Cycle-Consistent Adversarial Networks}}},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  date = {2020-08-24},
  eprint = {1703.10593},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1703.10593},
  url = {http://arxiv.org/abs/1703.10593},
  urldate = {2023-04-06},
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X \textbackslash rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y \textbackslash rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) \textbackslash approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {E\:\\data\\zotero\\storage\\IRH7U6T7\\Zhu 等 - 2020 - Unpaired Image-to-Image Translation using Cycle-Co.pdf;E\:\\data\\zotero\\storage\\P8TRUDSL\\1703.html}
}
