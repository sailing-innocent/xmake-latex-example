@online{barronMipNeRF360Unbounded2022,
  title = {Mip-{{NeRF}} 360: {{Unbounded Anti-Aliased Neural Radiance Fields}}},
  shorttitle = {Mip-{{NeRF}} 360},
  author = {Barron, Jonathan T. and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P. and Hedman, Peter},
  date = {2022-03-25},
  eprint = {2111.12077},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2111.12077},
  urldate = {2023-04-20},
  abstract = {Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on "unbounded" scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub "mip-NeRF 360" as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 57\% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\D5\\Zotero\\storage\\PZAMUU26\\Barron 等 - 2022 - Mip-NeRF 360 Unbounded Anti-Aliased Neural Radian.pdf;C\:\\Users\\D5\\Zotero\\storage\\74HMMP62\\2111.html}
}

@online{barronMipNeRFMultiscaleRepresentation2021,
  title = {Mip-{{NeRF}}: {{A Multiscale Representation}} for {{Anti-Aliasing Neural Radiance Fields}}},
  shorttitle = {Mip-{{NeRF}}},
  author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
  date = {2021-08-13},
  eprint = {2103.13415},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2103.13415},
  url = {http://arxiv.org/abs/2103.13415},
  urldate = {2023-04-20},
  abstract = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call "mip-NeRF" (a la "mipmap"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7\% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17\% on the dataset presented with NeRF and by 60\% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\D5\\Zotero\\storage\\DJ2UITVV\\Barron 等 - 2021 - Mip-NeRF A Multiscale Representation for Anti-Ali.pdf;C\:\\Users\\D5\\Zotero\\storage\\3IG3962K\\2103.html}
}

@online{barronZipNeRFAntiAliasedGridBased2023,
  title = {Zip-{{NeRF}}: {{Anti-Aliased Grid-Based Neural Radiance Fields}}},
  shorttitle = {Zip-{{NeRF}}},
  author = {Barron, Jonathan T. and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P. and Hedman, Peter},
  date = {2023-05-21},
  eprint = {2304.06706},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.06706},
  url = {http://arxiv.org/abs/2304.06706},
  urldate = {2023-07-04},
  abstract = {Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8\% - 77\% lower than either prior technique, and that trains 24x faster than mip-NeRF 360.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {C\:\\Users\\D5\\Zotero\\storage\\552STNIV\\Barron 等 - 2023 - Zip-NeRF Anti-Aliased Grid-Based Neural Radiance .pdf;C\:\\Users\\D5\\Zotero\\storage\\CBG733RH\\2304.html}
}

@article{blinnTextureReflectionComputer1976,
  title = {Texture and Reflection in Computer Generated Images},
  author = {Blinn, James F. and Newell, Martin E.},
  date = {1976-10-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {19},
  number = {10},
  pages = {542--547},
  issn = {0001-0782},
  doi = {10.1145/360349.360353},
  url = {https://dl.acm.org/doi/10.1145/360349.360353},
  urldate = {2023-07-04},
  abstract = {In 1974 Catmull developed a new algorithm for rendering images of bivariate surface patches. This paper describes extensions of this algorithm in the areas of texture simulation and lighting models. The parametrization of a patch defines a coordinate system which is used as a key for mapping patterns onto the surface. The intensity of the pattern at each picture element is computed as a weighted average of regions of the pattern definition function. The shape and size of this weighting function are chosen using digital signal processing theory. The patch rendering algorithm allows accurate computation of the surface normal to the patch at each picture element, permitting the simulation of mirror reflections. The amount of light coming from a given direction is modeled in a similar manner to the texture mapping and then added to the intensity obtained from the texture mapping. Several examples of images synthesized using these new techniques are included.},
  keywords = {computer graphics,graphic display,hidden surface removal,shading},
  file = {C\:\\Users\\D5\\Zotero\\storage\\KNYFZE42\\Blinn 和 Newell - 1976 - Texture and reflection in computer generated image.pdf}
}

@article{chenBidirectionalOpticalFlow2023,
  title = {Bidirectional {{Optical Flow NeRF}}: {{High Accuracy}} and {{High Quality}} under {{Fewer Views}}},
  shorttitle = {Bidirectional {{Optical Flow NeRF}}},
  author = {Chen, Shuo and Yan, Binbin and Sang, Xinzhu and Chen, Duo and Wang, Peng and Guo, Xiao and Zhong, Chongli and Wan, Huaming},
  date = {2023-06-26},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {37},
  number = {1},
  pages = {359--368},
  issn = {2374-3468},
  doi = {10.1609/aaai.v37i1.25109},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/25109},
  urldate = {2023-07-04},
  abstract = {Neural Radiance Fields (NeRF) can implicitly represent 3D-consistent RGB images and geometric by optimizing an underlying continuous volumetric scene function using a sparse set of input views, which has greatly benefited view synthesis tasks. However, NeRF fails to estimate correct geometry when given fewer views, resulting in failure to synthesize novel views. Existing works rely on introducing depth images or adding depth estimation networks to resolve the problem of poor synthetic view in NeRF with fewer views. However, due to the lack of spatial consistency of the single-depth image and the poor performance of depth estimation with fewer views, the existing methods still have challenges in addressing this problem. So this paper proposes Bidirectional Optical Flow NeRF(BOF-NeRF), which addresses this problem by mining optical flow information between 2D images. Our key insight is that utilizing 2D optical flow images to design a loss can effectively guide NeRF to learn the correct geometry and synthesize the right novel view. We also propose a view-enhanced fusion method based on geometry and color consistency to solve the problem of novel view details loss in NeRF. We conduct extensive experiments on the NeRF-LLFF and DTU MVS benchmarks for novel view synthesis tasks with fewer images in different complex real scenes. We further demonstrate the robustness of BOF-NeRF under different baseline distances on the Middlebury dataset. In all cases, BOF-NeRF outperforms current state-of-the-art baselines for novel view synthesis and scene geometry estimation.},
  issue = {1},
  langid = {english},
  keywords = {CV: Computational Photography,Image \& Video Synthesis},
  file = {C\:\\Users\\D5\\Zotero\\storage\\EK8628GJ\\Chen 等 - 2023 - Bidirectional Optical Flow NeRF High Accuracy and.pdf}
}

@online{chenReviewDeepLearningPowered2023,
  title = {A {{Review}} of {{Deep Learning-Powered Mesh Reconstruction Methods}}},
  author = {Chen, Zhiqin},
  date = {2023-03-05},
  eprint = {2303.02879},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.02879},
  urldate = {2023-07-03},
  abstract = {With the recent advances in hardware and rendering techniques, 3D models have emerged everywhere in our life. Yet creating 3D shapes is arduous and requires significant professional knowledge. Meanwhile, Deep learning has enabled high-quality 3D shape reconstruction from various sources, making it a viable approach to acquiring 3D shapes with minimal effort. Importantly, to be used in common 3D applications, the reconstructed shapes need to be represented as polygonal meshes, which is a challenge for neural networks due to the irregularity of mesh tessellations. In this survey, we provide a comprehensive review of mesh reconstruction methods that are powered by machine learning. We first describe various representations for 3D shapes in the deep learning context. Then we review the development of 3D mesh reconstruction methods from voxels, point clouds, single images, and multi-view images. Finally, we identify several challenges in this field and propose potential future directions.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {C\:\\Users\\D5\\Zotero\\storage\\YID5AXTP\\Chen - 2023 - A Review of Deep Learning-Powered Mesh Reconstruct.pdf;C\:\\Users\\D5\\Zotero\\storage\\SPXHVUND\\2303.html}
}

@inproceedings{corosDifferentiableSimulation2021,
  title = {Differentiable Simulation},
  booktitle = {{{SIGGRAPH Asia}} 2021 {{Courses}}},
  author = {Coros, Stelian and Macklin, Miles and Thomaszewski, Bernhard and Thürey, Nils},
  date = {2021-12-14},
  pages = {1--142},
  publisher = {{ACM}},
  location = {{Tokyo Japan}},
  doi = {10.1145/3476117.3483433},
  url = {https://dl.acm.org/doi/10.1145/3476117.3483433},
  urldate = {2023-07-04},
  eventtitle = {{{SA}} '21: {{SIGGRAPH Asia}} 2021},
  isbn = {978-1-4503-8684-5},
  langid = {english}
}

@online{dastjerdiEverLightIndoorOutdoorEditable2023,
  title = {{{EverLight}}: {{Indoor-Outdoor Editable HDR Lighting Estimation}}},
  shorttitle = {{{EverLight}}},
  author = {Dastjerdi, Mohammad Reza Karimi and Hold-Geoffroy, Yannick and Eisenmann, Jonathan and Lalonde, Jean-François},
  date = {2023-04-25},
  eprint = {2304.13207},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.13207},
  url = {http://arxiv.org/abs/2304.13207},
  urldate = {2023-07-04},
  abstract = {Because of the diversity in lighting environments, existing illumination estimation techniques have been designed explicitly on indoor or outdoor environments. Methods have focused specifically on capturing accurate energy (e.g., through parametric lighting models), which emphasizes shading and strong cast shadows; or producing plausible texture (e.g., with GANs), which prioritizes plausible reflections. Approaches which provide editable lighting capabilities have been proposed, but these tend to be with simplified lighting models, offering limited realism. In this work, we propose to bridge the gap between these recent trends in the literature, and propose a method which combines a parametric light model with 360\{\textbackslash deg\} panoramas, ready to use as HDRI in rendering engines. We leverage recent advances in GAN-based LDR panorama extrapolation from a regular image, which we extend to HDR using parametric spherical gaussians. To achieve this, we introduce a novel lighting co-modulation method that injects lighting-related features throughout the generator, tightly coupling the original or edited scene illumination within the panorama generation process. In our representation, users can easily edit light direction, intensity, number, etc. to impact shading while providing rich, complex reflections while seamlessly blending with the edits. Furthermore, our method encompasses indoor and outdoor environments, demonstrating state-of-the-art results even when compared to domain-specific methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{dastjerdiGuidedCoModulatedGAN2022,
  title = {Guided {{Co-Modulated GAN}} for 360\{\textbackslash deg\} {{Field}} of {{View Extrapolation}}},
  booktitle = {2022 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Dastjerdi, Mohammad Reza Karimi and Hold-Geoffroy, Yannick and Eisenmann, Jonathan and Khodadadeh, Siavash and Lalonde, Jean-François},
  date = {2022-09},
  eprint = {2204.07286},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {475--485},
  doi = {10.1109/3DV57658.2022.00059},
  url = {http://arxiv.org/abs/2204.07286},
  urldate = {2023-06-30},
  abstract = {We propose a method to extrapolate a 360\{\textbackslash deg\} field of view from a single image that allows for user-controlled synthesis of the out-painted content. To do so, we propose improvements to an existing GAN-based in-painting architecture for out-painting panoramic image representation. Our method obtains state-of-the-art results and outperforms previous methods on standard image quality metrics. To allow controlled synthesis of out-painting, we introduce a novel guided co-modulation framework, which drives the image generation process with a common pretrained discriminative model. Doing so maintains the high visual quality of generated panoramas while enabling user-controlled semantic content in the extrapolated field of view. We demonstrate the state-of-the-art results of our method on field of view extrapolation both qualitatively and quantitatively, providing thorough analysis of our novel editing capabilities. Finally, we demonstrate that our approach benefits the photorealistic virtual insertion of highly glossy objects in photographs.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\D5\\Zotero\\storage\\9GALBLTK\\Dastjerdi 等 - 2022 - Guided Co-Modulated GAN for 360 deg Field of Vie.pdf;C\:\\Users\\D5\\Zotero\\storage\\YXWQ5C6L\\2204.html}
}

@inproceedings{debevecModelingRenderingArchitecture1996a,
  title = {Modeling and Rendering Architecture from Photographs: A Hybrid Geometry- and Image-Based Approach},
  shorttitle = {Modeling and Rendering Architecture from Photographs},
  booktitle = {Proceedings of the 23rd Annual Conference on {{Computer}} Graphics and Interactive Techniques},
  author = {Debevec, Paul E. and Taylor, Camillo J. and Malik, Jitendra},
  date = {1996-08},
  pages = {11--20},
  publisher = {{ACM}},
  doi = {10.1145/237170.237191},
  url = {https://dl.acm.org/doi/10.1145/237170.237191},
  urldate = {2023-07-04},
  eventtitle = {{{SIGGRAPH96}}: 23rd {{International Conference}} on {{Computer Graphics}} and {{Interactive Techniques}}},
  isbn = {978-0-89791-746-9},
  langid = {english},
  file = {C\:\\Users\\D5\\Zotero\\storage\\AP2TRHYG\\Debevec 等 - 1996 - Modeling and rendering architecture from photograp.pdf}
}

@inproceedings{debevecRenderingSyntheticObjects1998,
  title = {Rendering Synthetic Objects into Real Scenes: Bridging Traditional and Image-Based Graphics with Global Illumination and High Dynamic Range Photography},
  shorttitle = {Rendering Synthetic Objects into Real Scenes},
  booktitle = {Proceedings of the 25th Annual Conference on {{Computer}} Graphics and Interactive Techniques},
  author = {Debevec, Paul},
  date = {1998-07-24},
  series = {{{SIGGRAPH}} '98},
  pages = {189--198},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/280814.280864},
  url = {https://dl.acm.org/doi/10.1145/280814.280864},
  urldate = {2023-06-26},
  isbn = {978-0-89791-999-9},
  file = {C\:\\Users\\D5\\Zotero\\storage\\5A98KQ6H\\Debevec - 1998 - Rendering synthetic objects into real scenes brid.pdf}
}

@article{durandFastBilateralFiltering2002,
  title = {Fast Bilateral Filtering for the Display of High-Dynamic-Range Images},
  author = {Durand, Frédo and Dorsey, Julie},
  date = {2002-07-01},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {21},
  number = {3},
  pages = {257--266},
  issn = {0730-0301},
  doi = {10.1145/566654.566574},
  url = {https://dl.acm.org/doi/10.1145/566654.566574},
  urldate = {2023-07-04},
  abstract = {We present a new technique for the display of high-dynamic-range images, which reduces the contrast while preserving detail. It is based on a two-scale decomposition of the image into a base layer, encoding large-scale variations, and a detail layer. Only the base layer has its contrast reduced, thereby preserving detail. The base layer is obtained using an edge-preserving filter called the bilateral filter. This is a non-linear filter, where the weight of each pixel is computed using a Gaussian in the spatial domain multiplied by an influence function in the intensity domain that decreases the weight of pixels with large intensity differences. We express bilateral filtering in the framework of robust statistics and show how it relates to anisotropic diffusion. We then accelerate bilateral filtering by using a piecewise-linear approximation in the intensity domain and appropriate subsampling. This results in a speed-up of two orders of magnitude. The method is fast and requires no parameter setting.},
  keywords = {contrast reduction,edge-preserving filtering,image processing,tone mapping,weird maths},
  file = {C\:\\Users\\D5\\Zotero\\storage\\UFZ4QH5L\\Durand 和 Dorsey - 2002 - Fast bilateral filtering for the display of high-d.pdf}
}

@online{fangOneAllBridging2023,
  title = {One Is {{All}}: {{Bridging}} the {{Gap Between Neural Radiance Fields Architectures}} with {{Progressive Volume Distillation}}},
  shorttitle = {One Is {{All}}},
  author = {Fang, Shuangkang and Xu, Weixin and Wang, Heng and Yang, Yi and Wang, Yufeng and Zhou, Shuchang},
  date = {2023-01-03},
  eprint = {2211.15977},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.15977},
  url = {http://arxiv.org/abs/2211.15977},
  urldate = {2023-07-04},
  abstract = {Neural Radiance Fields (NeRF) methods have proved effective as compact, high-quality and versatile representations for 3D scenes, and enable downstream tasks such as editing, retrieval, navigation, etc. Various neural architectures are vying for the core structure of NeRF, including the plain Multi-Layer Perceptron (MLP), sparse tensors, low-rank tensors, hashtables and their compositions. Each of these representations has its particular set of trade-offs. For example, the hashtable-based representations admit faster training and rendering but their lack of clear geometric meaning hampers downstream tasks like spatial-relation-aware editing. In this paper, we propose Progressive Volume Distillation (PVD), a systematic distillation method that allows any-to-any conversions between different architectures, including MLP, sparse or low-rank tensors, hashtables and their compositions. PVD consequently empowers downstream applications to optimally adapt the neural representations for the task at hand in a post hoc fashion. The conversions are fast, as distillation is progressively performed on different levels of volume representations, from shallower to deeper. We also employ special treatment of density to deal with its specific numerical instability problem. Empirical evidence is presented to validate our method on the NeRF-Synthetic, LLFF and TanksAndTemples datasets. For example, with PVD, an MLP-based NeRF model can be distilled from a hashtable-based Instant-NGP model at a 10X\textasciitilde 20X faster speed than being trained the original NeRF from scratch, while achieving a superior level of synthesis quality. Code is available at https://github.com/megvii-research/AAAI2023-PVD.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\D5\\Zotero\\storage\\QUA3ZFPS\\Fang 等 - 2023 - One is All Bridging the Gap Between Neural Radian.pdf;C\:\\Users\\D5\\Zotero\\storage\\G67LYDBN\\2211.html}
}

@article{fattalGradientDomainHigh2002,
  title = {Gradient Domain High Dynamic Range Compression},
  author = {Fattal, Raanan and Lischinski, Dani and Werman, Michael},
  date = {2002-07-01},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {21},
  number = {3},
  pages = {249--256},
  issn = {0730-0301},
  doi = {10.1145/566654.566573},
  url = {https://dl.acm.org/doi/10.1145/566654.566573},
  urldate = {2023-07-04},
  abstract = {We present a new method for rendering high dynamic range images on conventional displays. Our method is conceptually simple, computationally efficient, robust, and easy to use. We manipulate the gradient field of the luminance image by attenuating the magnitudes of large gradients. A new, low dynamic range image is then obtained by solving a Poisson equation on the modified gradient field. Our results demonstrate that the method is capable of drastic dynamic range compression, while preserving fine details and avoiding common artifacts, such as halos, gradient reversals, or loss of local contrast. The method is also able to significantly enhance ordinary images by bringing out detail in dark regions.},
  keywords = {digital photography,high dynamic range compression,image processing,image-based rendering,signal processing,tone mapping},
  file = {C\:\\Users\\D5\\Zotero\\storage\\ZJQ8KCKL\\Fattal 等 - 2002 - Gradient domain high dynamic range compression.pdf}
}

@online{gardnerLearningPredictIndoor2017,
  title = {Learning to {{Predict Indoor Illumination}} from a {{Single Image}}},
  author = {Gardner, Marc-André and Sunkavalli, Kalyan and Yumer, Ersin and Shen, Xiaohui and Gambaretto, Emiliano and Gagné, Christian and Lalonde, Jean-François},
  date = {2017-11-21},
  eprint = {1704.00090},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1704.00090},
  url = {http://arxiv.org/abs/1704.00090},
  urldate = {2023-06-30},
  abstract = {We propose an automatic method to infer high dynamic range illumination from a single, limited field-of-view, low dynamic range photograph of an indoor scene. In contrast to previous work that relies on specialized image capture, user input, and/or simple scene models, we train an end-to-end deep neural network that directly regresses a limited field-of-view photo to HDR illumination, without strong assumptions on scene geometry, material properties, or lighting. We show that this can be accomplished in a three step process: 1) we train a robust lighting classifier to automatically annotate the location of light sources in a large dataset of LDR environment maps, 2) we use these annotations to train a deep neural network that predicts the location of lights in a scene from a single limited field-of-view photo, and 3) we fine-tune this network using a small dataset of HDR environment maps to predict light intensities. This allows us to automatically recover high-quality HDR illumination estimates that significantly outperform previous state-of-the-art methods. Consequently, using our illumination estimates for applications like 3D object insertion, we can achieve results that are photo-realistic, which is validated via a perceptual user study.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Statistics - Machine Learning},
  file = {C\:\\Users\\D5\\Zotero\\storage\\6QXXLC8S\\Gardner 等 - 2017 - Learning to Predict Indoor Illumination from a Sin.pdf;C\:\\Users\\D5\\Zotero\\storage\\UMZS2H82\\1704.html}
}

@article{hold-geoffroyDeepSkyModeling2019,
  title = {Deep {{Sky Modeling}} for {{Single Image Outdoor Lighting Estimation}}},
  author = {Hold-Geoffroy, Yannick and Athawale, Akshaya and Lalonde, Jean-Francois},
  date = {2019-06},
  journaltitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {6920--6928},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00709},
  url = {https://ieeexplore.ieee.org/document/8954446/},
  urldate = {2023-03-06},
  abstract = {We propose a data-driven learned sky model, which we use for outdoor lighting estimation from a single image. As no large-scale dataset of images and their corresponding ground truth illumination is readily available, we use complementary datasets to train our approach, combining the vast diversity of illumination conditions of SUN360 with the radiometrically calibrated and physically accurate Laval HDR sky database. Our key contribution is to provide a holistic view of both lighting modeling and estimation, solving both problems end-to-end. From a test image, our method can directly estimate an HDR environment map of the lighting without relying on analytical lighting models. We demonstrate the versatility and expressivity of our learned sky model and show that it can be used to recover plausible illumination, leading to visually pleasant virtual object insertions. To further evaluate our method, we capture a dataset of HDR 360° panoramas and show through extensive validation that we significantly outperform previous state-of-the-art.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {9781728132938},
  file = {C\:\\Users\\D5\\Zotero\\storage\\XPX3UFZ8\\Hold-Geoffroy 等 - 2019 - Deep Sky Modeling for Single Image Outdoor Lightin.pdf;D\:\\data\\papers\\deep-sky-modeling-for-single-image-outdoor-lighting-estimation.pdf}
}

@online{katoDifferentiableRenderingSurvey2020,
  title = {Differentiable {{Rendering}}: {{A Survey}}},
  shorttitle = {Differentiable {{Rendering}}},
  author = {Kato, Hiroharu and Beker, Deniz and Morariu, Mihai and Ando, Takahiro and Matsuoka, Toru and Kehl, Wadim and Gaidon, Adrien},
  date = {2020-07-30},
  eprint = {2006.12057},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2006.12057},
  urldate = {2023-07-03},
  abstract = {Deep neural networks (DNNs) have shown remarkable performance improvements on vision-related tasks such as object detection or image segmentation. Despite their success, they generally lack the understanding of 3D objects which form the image, as it is not always possible to collect 3D information about the scene or to easily annotate it. Differentiable rendering is a novel field which allows the gradients of 3D objects to be calculated and propagated through images. It also reduces the requirement of 3D data collection and annotation, while enabling higher success rate in various applications. This paper reviews existing literature and discusses the current state of differentiable rendering, its applications and open research problems.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\D5\\Zotero\\storage\\2IAHINKW\\Kato 等 - 2020 - Differentiable Rendering A Survey.pdf;C\:\\Users\\D5\\Zotero\\storage\\PK3796DQ\\2006.html}
}

@online{katoNeural3DMesh2017,
  title = {Neural {{3D Mesh Renderer}}},
  author = {Kato, Hiroharu and Ushiku, Yoshitaka and Harada, Tatsuya},
  date = {2017-11-20},
  eprint = {1711.07566},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1711.07566},
  url = {http://arxiv.org/abs/1711.07566},
  urldate = {2023-07-03},
  abstract = {For modeling the 3D world behind 2D images, which 3D representation is most appropriate? A polygon mesh is a promising candidate for its compactness and geometric properties. However, it is not straightforward to model a polygon mesh from 2D images using neural networks because the conversion from a mesh to an image, or rendering, involves a discrete operation called rasterization, which prevents back-propagation. Therefore, in this work, we propose an approximate gradient for rasterization that enables the integration of rendering into neural networks. Using this renderer, we perform single-image 3D mesh reconstruction with silhouette image supervision and our system outperforms the existing voxel-based approach. Additionally, we perform gradient-based 3D mesh editing operations, such as 2D-to-3D style transfer and 3D DeepDream, with 2D supervision for the first time. These applications demonstrate the potential of the integration of a mesh renderer into neural networks and the effectiveness of our proposed renderer.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\D5\\Zotero\\storage\\8R6HJ978\\Kato 等 - 2017 - Neural 3D Mesh Renderer.pdf;C\:\\Users\\D5\\Zotero\\storage\\FBDYEFRX\\1711.html}
}

@inproceedings{millerIlluminationReflectionMaps1984,
  title = {Illumination and {{Reflection Maps}} : {{Simulated Objects}} in {{Simulated}} and {{Real Environments Gene}}},
  shorttitle = {Illumination and {{Reflection Maps}}},
  author = {Miller, S. and Synthavision, Magi and Hoffman, C. R.},
  date = {1984},
  url = {https://www.semanticscholar.org/paper/Illumination-and-Reflection-Maps-%3A-Simulated-in-and-Miller-Synthavision/1026a49094e21fa79ef9791a4dfdd5704294564e},
  urldate = {2023-07-04},
  abstract = {Blinn and Newell introduced reflection maps for computer simulated mirror highlights. This paper extends their method to cover a wider class of reflectance models. Panoramic images of real, painted and simulated environments are used as illumination maps that are convolved (blurred) and transformed to create reflection maps. These tables of reflected light values are used to efficiently shade objects in an animation sequence. Shaders based on point illumination may be improved in a straightforward manner to use reflection maps. Shading is by table-lookup, and the number of calculations per pixel is constant regardless of the complexity of the reflected scene. Antialiased mapping further improves image quality. The resulting pictures have many of the reality cues associated with ray-tracing but at greatly reduced computational cost. The geometry of highlights is less exact than in ray-tracing, and multiple surface reflections are not explicitly handled. The color of diffuse reflections can be rendered more accurately than in ray-tracing. CR},
  file = {C\:\\Users\\D5\\Zotero\\storage\\TTVV3YVS\\Miller 等 - 1984 - Illumination and Reflection Maps  Simulated Objec.pdf}
}

@article{nimier-davidUnbiasedInverseVolume2022,
  title = {Unbiased Inverse Volume Rendering with Differential Trackers},
  author = {Nimier-David, Merlin and Müller, Thomas and Keller, Alexander and Jakob, Wenzel},
  date = {2022-07-22},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {41},
  number = {4},
  pages = {44:1--44:20},
  issn = {0730-0301},
  doi = {10.1145/3528223.3530073},
  url = {https://dl.acm.org/doi/10.1145/3528223.3530073},
  urldate = {2023-07-04},
  abstract = {Volumetric representations are popular in inverse rendering because they have a simple parameterization, are smoothly varying, and transparently handle topology changes. However, incorporating the full volumetric transport of light is costly and challenging, often leading practitioners to implement simplified models, such as purely emissive and absorbing volumes with "baked" lighting. One such challenge is the efficient estimation of the gradients of the volume's appearance with respect to its scattering and absorption parameters. We show that the straightforward approach---differentiating a volumetric free-flight sampler---can lead to biased and high-variance gradients, hindering optimization. Instead, we propose using a new sampling strategy: differential ratio tracking, which is unbiased, yields low-variance gradients, and runs in linear time. Differential ratio tracking combines ratio tracking and reservoir sampling to estimate gradients by sampling distances proportional to the unweighted transmittance rather than the usual extinction-weighted transmittance. In addition, we observe local minima when optimizing scattering parameters to reproduce dense volumes or surfaces. We show that these local minima can be overcome by bootstrapping the optimization from nonphysical emissive volumes that are easily optimized.},
  keywords = {differentiable rendering,importance sampling,inverse rendering,radiative backpropagation,volumetric rendering},
  file = {C\:\\Users\\D5\\Zotero\\storage\\77WVZ8WV\\Nimier-David 等 - 2022 - Unbiased inverse volume rendering with differentia.pdf}
}

@online{panGAN2XNonLambertianInverse2022,
  title = {{{GAN2X}}: {{Non-Lambertian Inverse Rendering}} of {{Image GANs}}},
  shorttitle = {{{GAN2X}}},
  author = {Pan, Xingang and Tewari, Ayush and Liu, Lingjie and Theobalt, Christian},
  date = {2022-11-14},
  eprint = {2206.09244},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2206.09244},
  urldate = {2023-07-04},
  abstract = {2D images are observations of the 3D physical world depicted with the geometry, material, and illumination components. Recovering these underlying intrinsic components from 2D images, also known as inverse rendering, usually requires a supervised setting with paired images collected from multiple viewpoints and lighting conditions, which is resource-demanding. In this work, we present GAN2X, a new method for unsupervised inverse rendering that only uses unpaired images for training. Unlike previous Shape-from-GAN approaches that mainly focus on 3D shapes, we take the first attempt to also recover non-Lambertian material properties by exploiting the pseudo paired data generated by a GAN. To achieve precise inverse rendering, we devise a specularity-aware neural surface representation that continuously models the geometry and material properties. A shading-based refinement technique is adopted to further distill information in the target image and recover more fine details. Experiments demonstrate that GAN2X can accurately decompose 2D images to 3D shape, albedo, and specular properties for different object categories, and achieves the state-of-the-art performance for unsupervised single-view 3D face reconstruction. We also show its applications in downstream tasks including real image editing and lifting 2D GANs to decomposed 3D GANs.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\D5\\Zotero\\storage\\DVYHY8CB\\Pan 等 - 2022 - GAN2X Non-Lambertian Inverse Rendering of Image G.pdf;C\:\\Users\\D5\\Zotero\\storage\\S9W9GWDD\\2206.html}
}

@article{reinhardPhotographicToneReproduction2002,
  title = {Photographic Tone Reproduction for Digital Images},
  author = {Reinhard, Erik and Stark, Michael and Shirley, Peter and Ferwerda, James},
  date = {2002-07-01},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {21},
  number = {3},
  pages = {267--276},
  issn = {0730-0301},
  doi = {10.1145/566654.566575},
  url = {https://dl.acm.org/doi/10.1145/566654.566575},
  urldate = {2023-07-04},
  abstract = {A classic photographic task is the mapping of the potentially high dynamic range of real world luminances to the low dynamic range of the photographic print. This tone reproduction problem is also faced by computer graphics practitioners who map digital images to a low dynamic range print or screen. The work presented in this paper leverages the time-tested techniques of photographic practice to develop a new tone reproduction operator. In particular, we use and extend the techniques developed by Ansel Adams to deal with digital images. The resulting algorithm is simple and produces good results for a wide variety of images.},
  keywords = {dynamic range,tone reproduction,zone system},
  file = {C\:\\Users\\D5\\Zotero\\storage\\P7UVXTGX\\Reinhard 等 - 2002 - Photographic tone reproduction for digital images.pdf}
}

@article{rempelLdr2HdrOntheflyReverse2007,
  title = {{{Ldr2Hdr}}: On-the-Fly Reverse Tone Mapping of Legacy Video and Photographs},
  shorttitle = {{{Ldr2Hdr}}},
  author = {Rempel, Allan G. and Trentacoste, Matthew and Seetzen, Helge and Young, H. David and Heidrich, Wolfgang and Whitehead, Lorne and Ward, Greg},
  date = {2007-07-29},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {26},
  number = {3},
  pages = {39--es},
  issn = {0730-0301},
  doi = {10.1145/1276377.1276426},
  url = {https://dl.acm.org/doi/10.1145/1276377.1276426},
  urldate = {2023-07-04},
  abstract = {New generations of display devices promise to provide significantly improved dynamic range over conventional display technology. In the long run, evolving camera technology and file formats will provide high fidelity content for these display devices. In the near term, however, the vast majority of images and video will only be available in low dynamic range formats. In this paper we describe a method for boosting the dynamic range of legacy video and photographs for viewing on high dynamic range displays. Our emphasis is on real-time processing of video streams, such as web streams or the signal from a DVD player. We place particular emphasis on robustness of the method, and its ability to deal with a wide range of content without user adjusted parameters or visible artifacts. The method can be implemented on both graphics hardware and on signal processors that are directly integrated in the HDR displays.},
  keywords = {image and video processing - high dynamic range/tone mapping,methods and applications - signal processing},
  file = {C\:\\Users\\D5\\Zotero\\storage\\DYQAWBE8\\Rempel 等 - 2007 - Ldr2Hdr on-the-fly reverse tone mapping of legacy.pdf}
}

@online{wangLearningIndoorInverse2021,
  title = {Learning {{Indoor Inverse Rendering}} with {{3D Spatially-Varying Lighting}}},
  author = {Wang, Zian and Philion, Jonah and Fidler, Sanja and Kautz, Jan},
  date = {2021-10-20},
  eprint = {2109.06061},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2109.06061},
  url = {http://arxiv.org/abs/2109.06061},
  urldate = {2023-07-04},
  abstract = {In this work, we address the problem of jointly estimating albedo, normals, depth and 3D spatially-varying lighting from a single image. Most existing methods formulate the task as image-to-image translation, ignoring the 3D properties of the scene. However, indoor scenes contain complex 3D light transport where a 2D representation is insufficient. In this paper, we propose a unified, learning-based inverse rendering framework that formulates 3D spatially-varying lighting. Inspired by classic volume rendering techniques, we propose a novel Volumetric Spherical Gaussian representation for lighting, which parameterizes the exitant radiance of the 3D scene surfaces on a voxel grid. We design a physics based differentiable renderer that utilizes our 3D lighting representation, and formulates the energy-conserving image formation process that enables joint training of all intrinsic properties with the re-rendering constraint. Our model ensures physically correct predictions and avoids the need for ground-truth HDR lighting which is not easily accessible. Experiments show that our method outperforms prior works both quantitatively and qualitatively, and is capable of producing photorealistic results for AR applications such as virtual object insertion even for highly specular objects.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\D5\\Zotero\\storage\\EFS53NYJ\\Wang 等 - 2021 - Learning Indoor Inverse Rendering with 3D Spatiall.pdf;C\:\\Users\\D5\\Zotero\\storage\\BUKKIG2L\\2109.html}
}

@online{wangStyleLightHDRPanorama2022,
  title = {{{StyleLight}}: {{HDR Panorama Generation}} for {{Lighting Estimation}} and {{Editing}}},
  shorttitle = {{{StyleLight}}},
  author = {Wang, Guangcong and Yang, Yinuo and Loy, Chen Change and Liu, Ziwei},
  date = {2022-07-29},
  eprint = {2207.14811},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.14811},
  url = {http://arxiv.org/abs/2207.14811},
  urldate = {2023-06-30},
  abstract = {We present a new lighting estimation and editing framework to generate high-dynamic-range (HDR) indoor panorama lighting from a single limited field-of-view (LFOV) image captured by low-dynamic-range (LDR) cameras. Existing lighting estimation methods either directly regress lighting representation parameters or decompose this problem into LFOV-to-panorama and LDR-to-HDR lighting generation sub-tasks. However, due to the partial observation, the high-dynamic-range lighting, and the intrinsic ambiguity of a scene, lighting estimation remains a challenging task. To tackle this problem, we propose a coupled dual-StyleGAN panorama synthesis network (StyleLight) that integrates LDR and HDR panorama synthesis into a unified framework. The LDR and HDR panorama synthesis share a similar generator but have separate discriminators. During inference, given an LDR LFOV image, we propose a focal-masked GAN inversion method to find its latent code by the LDR panorama synthesis branch and then synthesize the HDR panorama by the HDR panorama synthesis branch. StyleLight takes LFOV-to-panorama and LDR-to-HDR lighting generation into a unified framework and thus greatly improves lighting estimation. Extensive experiments demonstrate that our framework achieves superior performance over state-of-the-art methods on indoor lighting estimation. Notably, StyleLight also enables intuitive lighting editing on indoor HDR panoramas, which is suitable for real-world applications. Code is available at https://style-light.github.io.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\D5\\Zotero\\storage\\2GDX9TCM\\Wang 等 - 2022 - StyleLight HDR Panorama Generation for Lighting E.pdf;C\:\\Users\\D5\\Zotero\\storage\\H6TFU99M\\2207.html}
}

@inproceedings{wardHighDynamicRange2008,
  title = {High Dynamic Range Imaging \& Image-Based Lighting},
  booktitle = {{{ACM SIGGRAPH}} 2008 Classes},
  author = {Ward, Greg and Reinhard, Erik and Debevec, Paul},
  date = {2008-08-11},
  series = {{{SIGGRAPH}} '08},
  pages = {1--137},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1401132.1401170},
  url = {https://doi.org/10.1145/1401132.1401170},
  urldate = {2023-07-03},
  abstract = {This class outlines recent advances in high dynamic range imaging (HDRI) - from capture to image-based lighting to display. In a hands-on approach, we show how HDR images and video can be captured, the file formats available to store them, and the algorithms required to prepare them for display on low dynamic range displays. The trade-offs at each step are assessed allowing attendees to make informed choices about data capture techniques, file formats and tone reproduction operators. In addition, the latest developments in image-based lighting will be presented.},
  isbn = {978-1-4503-7845-1}
}

@online{wimbauerDerendering3DObjects2022,
  title = {De-Rendering {{3D Objects}} in the {{Wild}}},
  author = {Wimbauer, Felix and Wu, Shangzhe and Rupprecht, Christian},
  date = {2022-01-06},
  url = {https://arxiv.org/abs/2201.02279v2},
  urldate = {2023-07-03},
  abstract = {With increasing focus on augmented and virtual reality applications (XR) comes the demand for algorithms that can lift objects from images and videos into representations that are suitable for a wide variety of related 3D tasks. Large-scale deployment of XR devices and applications means that we cannot solely rely on supervised learning, as collecting and annotating data for the unlimited variety of objects in the real world is infeasible. We present a weakly supervised method that is able to decompose a single image of an object into shape (depth and normals), material (albedo, reflectivity and shininess) and global lighting parameters. For training, the method only relies on a rough initial shape estimate of the training objects to bootstrap the learning process. This shape supervision can come for example from a pretrained depth network or - more generically - from a traditional structure-from-motion pipeline. In our experiments, we show that the method can successfully de-render 2D images into a decomposed 3D representation and generalizes to unseen object categories. Since in-the-wild evaluation is difficult due to the lack of ground truth data, we also introduce a photo-realistic synthetic test set that allows for quantitative evaluation.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {C\:\\Users\\D5\\Zotero\\storage\\4XNF93YY\\Wimbauer 等 - 2022 - De-rendering 3D Objects in the Wild.pdf}
}

@online{wuUnsupervisedLearningProbably2020,
  title = {Unsupervised {{Learning}} of {{Probably Symmetric Deformable 3D Objects}} from {{Images}} in the {{Wild}}},
  author = {Wu, Shangzhe and Rupprecht, Christian and Vedaldi, Andrea},
  date = {2020-03-30},
  eprint = {1911.11130},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1911.11130},
  url = {http://arxiv.org/abs/1911.11130},
  urldate = {2023-07-03},
  abstract = {We propose a method to learn 3D deformable object categories from raw single-view images, without external supervision. The method is based on an autoencoder that factors each input image into depth, albedo, viewpoint and illumination. In order to disentangle these components without supervision, we use the fact that many object categories have, at least in principle, a symmetric structure. We show that reasoning about illumination allows us to exploit the underlying object symmetry even if the appearance is not symmetric due to shading. Furthermore, we model objects that are probably, but not certainly, symmetric by predicting a symmetry probability map, learned end-to-end with the other components of the model. Our experiments show that this method can recover very accurately the 3D shape of human faces, cat faces and cars from single-view images, without any supervision or a prior shape model. On benchmarks, we demonstrate superior accuracy compared to another method that uses supervision at the level of 2D image correspondences.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\D5\\Zotero\\storage\\QCFLDFGI\\Wu 等 - 2020 - Unsupervised Learning of Probably Symmetric Deform.pdf;C\:\\Users\\D5\\Zotero\\storage\\ZXXEGUBL\\1911.html}
}

@online{yanHORIZONHighResolutionPanorama2022,
  title = {{{HORIZON}}: {{A High-Resolution Panorama Synthesis Framework}}},
  shorttitle = {{{HORIZON}}},
  author = {Yan, Kun and Ji, Lei and Wu, Chenfei and Liang, Jian and Zhou, Ming and Duan, Nan and Ma, Shuai},
  date = {2022-10-10},
  eprint = {2210.04522},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.04522},
  url = {http://arxiv.org/abs/2210.04522},
  urldate = {2023-06-30},
  abstract = {Panorama synthesis aims to generate a visual scene with all 360-degree views and enables an immersive virtual world. If the panorama synthesis process can be semantically controlled, we can then build an interactive virtual world and form an unprecedented human-computer interaction experience. Existing panoramic synthesis methods mainly focus on dealing with the inherent challenges brought by panoramas' spherical structure such as the projection distortion and the in-continuity problem when stitching edges, but is hard to effectively control semantics. The recent success of visual synthesis like DALL.E generates promising 2D flat images with semantic control, however, it is hard to directly be applied to panorama synthesis which inevitably generates distorted content. Besides, both of the above methods can not effectively synthesize high-resolution panoramas either because of quality or inference speed. In this work, we propose a new generation framework for high-resolution panorama images. The contributions include 1) alleviating the spherical distortion and edge in-continuity problem through spherical modeling, 2) supporting semantic control through both image and text hints, and 3) effectively generating high-resolution panoramas through parallel decoding. Our experimental results on a large-scale high-resolution Street View dataset validated the superiority of our approach quantitatively and qualitatively.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\D5\\Zotero\\storage\\HCPEY6ZT\\Yan 等 - 2022 - HORIZON A High-Resolution Panorama Synthesis Fram.pdf;C\:\\Users\\D5\\Zotero\\storage\\CQPLXH2G\\2210.html}
}

@online{yaoNeILFNeuralIncident2022,
  title = {{{NeILF}}: {{Neural Incident Light Field}} for {{Physically-based Material Estimation}}},
  shorttitle = {{{NeILF}}},
  author = {Yao, Yao and Zhang, Jingyang and Liu, Jingbo and Qu, Yihang and Fang, Tian and McKinnon, David and Tsin, Yanghai and Quan, Long},
  date = {2022-03-18},
  eprint = {2203.07182},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.07182},
  urldate = {2023-07-03},
  abstract = {We present a differentiable rendering framework for material and lighting estimation from multi-view images and a reconstructed geometry. In the framework, we represent scene lightings as the Neural Incident Light Field (NeILF) and material properties as the surface BRDF modelled by multi-layer perceptrons. Compared with recent approaches that approximate scene lightings as the 2D environment map, NeILF is a fully 5D light field that is capable of modelling illuminations of any static scenes. In addition, occlusions and indirect lights can be handled naturally by the NeILF representation without requiring multiple bounces of ray tracing, making it possible to estimate material properties even for scenes with complex lightings and geometries. We also propose a smoothness regularization and a Lambertian assumption to reduce the material-lighting ambiguity during the optimization. Our method strictly follows the physically-based rendering equation, and jointly optimizes material and lighting through the differentiable rendering process. We have intensively evaluated the proposed method on our in-house synthetic dataset, the DTU MVS dataset, and real-world BlendedMVS scenes. Our method is able to outperform previous methods by a significant margin in terms of novel view rendering quality, setting a new state-of-the-art for image-based material and lighting estimation.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\D5\\Zotero\\storage\\JIHXKFWK\\Yao 等 - 2022 - NeILF Neural Incident Light Field for Physically-.pdf;C\:\\Users\\D5\\Zotero\\storage\\AHX2QB8V\\2203.html}
}

@online{yuInverseRenderNetLearningSingle2018,
  title = {{{InverseRenderNet}}: {{Learning}} Single Image Inverse Rendering},
  shorttitle = {{{InverseRenderNet}}},
  author = {Yu, Ye and Smith, William A. P.},
  date = {2018-11-29},
  eprint = {1811.12328},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1811.12328},
  urldate = {2023-07-04},
  abstract = {We show how to train a fully convolutional neural network to perform inverse rendering from a single, uncontrolled image. The network takes an RGB image as input, regresses albedo and normal maps from which we compute lighting coefficients. Our network is trained using large uncontrolled image collections without ground truth. By incorporating a differentiable renderer, our network can learn from self-supervision. Since the problem is ill-posed we introduce additional supervision: 1. We learn a statistical natural illumination prior, 2. Our key insight is to perform offline multiview stereo (MVS) on images containing rich illumination variation. From the MVS pose and depth maps, we can cross project between overlapping views such that Siamese training can be used to ensure consistent estimation of photometric invariants. MVS depth also provides direct coarse supervision for normal map estimation. We believe this is the first attempt to use MVS supervision for learning inverse rendering.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\D5\\Zotero\\storage\\TG6J9QX7\\Yu 和 Smith - 2018 - InverseRenderNet Learning single image inverse re.pdf;C\:\\Users\\D5\\Zotero\\storage\\LYYDWM33\\1811.html}
}

@online{yuOutdoorInverseRendering2021,
  title = {Outdoor Inverse Rendering from a Single Image Using Multiview Self-Supervision},
  author = {Yu, Ye and Smith, William A. P.},
  date = {2021-02-12},
  eprint = {2102.06591},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2102.06591},
  urldate = {2023-07-04},
  abstract = {In this paper we show how to perform scene-level inverse rendering to recover shape, reflectance and lighting from a single, uncontrolled image using a fully convolutional neural network. The network takes an RGB image as input, regresses albedo, shadow and normal maps from which we infer least squares optimal spherical harmonic lighting coefficients. Our network is trained using large uncontrolled multiview and timelapse image collections without ground truth. By incorporating a differentiable renderer, our network can learn from self-supervision. Since the problem is ill-posed we introduce additional supervision. Our key insight is to perform offline multiview stereo (MVS) on images containing rich illumination variation. From the MVS pose and depth maps, we can cross project between overlapping views such that Siamese training can be used to ensure consistent estimation of photometric invariants. MVS depth also provides direct coarse supervision for normal map estimation. We believe this is the first attempt to use MVS supervision for learning inverse rendering. In addition, we learn a statistical natural illumination prior. We evaluate performance on inverse rendering, normal map estimation and intrinsic image decomposition benchmarks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\D5\\Zotero\\storage\\BGRM73PD\\Yu 和 Smith - 2021 - Outdoor inverse rendering from a single image usin.pdf;C\:\\Users\\D5\\Zotero\\storage\\6BQIFA24\\2102.html}
}

@online{zhanEMLightLightingEstimation2020,
  title = {{{EMLight}}: {{Lighting Estimation}} via {{Spherical Distribution Approximation}}},
  shorttitle = {{{EMLight}}},
  author = {Zhan, Fangneng and Zhang, Changgong and Yu, Yingchen and Chang, Yuan and Lu, Shijian and Ma, Feiying and Xie, Xuansong},
  date = {2020-12-20},
  eprint = {2012.11116},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.11116},
  url = {http://arxiv.org/abs/2012.11116},
  urldate = {2023-07-03},
  abstract = {Illumination estimation from a single image is critical in 3D rendering and it has been investigated extensively in the computer vision and computer graphic research community. On the other hand, existing works estimate illumination by either regressing light parameters or generating illumination maps that are often hard to optimize or tend to produce inaccurate predictions. We propose Earth Mover Light (EMLight), an illumination estimation framework that leverages a regression network and a neural projector for accurate illumination estimation. We decompose the illumination map into spherical light distribution, light intensity and the ambient term, and define the illumination estimation as a parameter regression task for the three illumination components. Motivated by the Earth Mover distance, we design a novel spherical mover's loss that guides to regress light distribution parameters accurately by taking advantage of the subtleties of spherical distribution. Under the guidance of the predicted spherical distribution, light intensity and ambient term, the neural projector synthesizes panoramic illumination maps with realistic light frequency. Extensive experiments show that EMLight achieves accurate illumination estimation and the generated relighting in 3D object embedding exhibits superior plausibility and fidelity as compared with state-of-the-art methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\D5\\Zotero\\storage\\VEQUN357\\Zhan 等 - 2020 - EMLight Lighting Estimation via Spherical Distrib.pdf;C\:\\Users\\D5\\Zotero\\storage\\UHTGG42E\\2012.html}
}

@inproceedings{zhangAllWeatherDeepOutdoor2019,
  title = {All-{{Weather Deep Outdoor Lighting Estimation}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhang, Jinsong and Sunkavalli, Kalyan and Hold-Geoffroy, Yannick and Hadap, Sunil and Eisenman, Jonathan and Lalonde, Jean-François},
  date = {2019-06},
  pages = {10150--10158},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.01040},
  abstract = {We present a neural network that predicts HDR outdoor illumination from a single LDR image. At the heart of our work is a method to accurately learn HDR lighting from LDR panoramas under any weather condition. We achieve this by training another CNN (on a combination of synthetic and real images) to take as input an LDR panorama, and regress the parameters of the Lalonde-Mathews outdoor illumination model. This model is trained such that it a) reconstructs the appearance of the sky, and b) renders the appearance of objects lit by this illumination. We use this network to label a large-scale dataset of LDR panoramas with lighting parameters and use them to train our single image outdoor lighting estimation network. We demonstrate, via extensive experiments, that both our panorama and singe image networks outperform the state of the art, and unlike prior work, are able to handle weather conditions ranging from fully sunny to overcast skies.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {Computational Photography,Deep Learning,Distance measurement,Estimation,Heart,Lighting,Neural networks,Pattern recognition,Training,Vision + Graphics},
  file = {C\:\\Users\\D5\\Zotero\\storage\\D7E2M85X\\Zhang 等 - 2019 - All-Weather Deep Outdoor Lighting Estimation.pdf;C\:\\Users\\D5\\Zotero\\storage\\V4FYSCVY\\Zhang 等 - 2019 - All-Weather Deep Outdoor Lighting Estimation.pdf;D\:\\data\\papers\\all-weather-deep-outdoor-lighting-estimation.pdf;C\:\\Users\\D5\\Zotero\\storage\\IQ9R6AXT\\8953336.html}
}

@article{zhanGMLightLightingEstimation2022,
  title = {{{GMLight}}: {{Lighting Estimation}} via {{Geometric Distribution Approximation}}},
  shorttitle = {{{GMLight}}},
  author = {Zhan, Fangneng and Yu, Yingchen and Zhang, Changgong and Wu, Rongliang and Hu, Wenbo and Lu, Shijian and Ma, Feiying and Xie, Xuansong and Shao, Ling},
  date = {2022},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {31},
  eprint = {2102.10244},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {2268--2278},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2022.3151997},
  url = {http://arxiv.org/abs/2102.10244},
  urldate = {2023-07-03},
  abstract = {Inferring the scene illumination from a single image is an essential yet challenging task in computer vision and computer graphics. Existing works estimate lighting by regressing representative illumination parameters or generating illumination maps directly. However, these methods often suffer from poor accuracy and generalization. This paper presents Geometric Mover's Light (GMLight), a lighting estimation framework that employs a regression network and a generative projector for effective illumination estimation. We parameterize illumination scenes in terms of the geometric light distribution, light intensity, ambient term, and auxiliary depth, which can be estimated by a regression network. Inspired by the earth mover's distance, we design a novel geometric mover's loss to guide the accurate regression of light distribution parameters. With the estimated light parameters, the generative projector synthesizes panoramic illumination maps with realistic appearance and high-frequency details. Extensive experiments show that GMLight achieves accurate illumination estimation and superior fidelity in relighting for 3D object insertion. The codes are available at \textbackslash href\{https://github.com/fnzhan/Illumination-Estimation\}\{https://github.com/fnzhan/Illumination-Estimation\}.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\D5\\Zotero\\storage\\JZ8XM75R\\Zhan 等 - 2022 - GMLight Lighting Estimation via Geometric Distrib.pdf;C\:\\Users\\D5\\Zotero\\storage\\E7HUUH3N\\2102.html}
}

@online{zhangModelingIndirectIllumination2022,
  title = {Modeling {{Indirect Illumination}} for {{Inverse Rendering}}},
  author = {Zhang, Yuanqing and Sun, Jiaming and He, Xingyi and Fu, Huan and Jia, Rongfei and Zhou, Xiaowei},
  date = {2022-04-14},
  eprint = {2204.06837},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2204.06837},
  urldate = {2023-07-04},
  abstract = {Recent advances in implicit neural representations and differentiable rendering make it possible to simultaneously recover the geometry and materials of an object from multi-view RGB images captured under unknown static illumination. Despite the promising results achieved, indirect illumination is rarely modeled in previous methods, as it requires expensive recursive path tracing which makes the inverse rendering computationally intractable. In this paper, we propose a novel approach to efficiently recovering spatially-varying indirect illumination. The key insight is that indirect illumination can be conveniently derived from the neural radiance field learned from input images instead of being estimated jointly with direct illumination and materials. By properly modeling the indirect illumination and visibility of direct illumination, interreflection- and shadow-free albedo can be recovered. The experiments on both synthetic and real data demonstrate the superior performance of our approach compared to previous work and its capability to synthesize realistic renderings under novel viewpoints and illumination. Our code and data are available at https://zju3dv.github.io/invrender/.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\D5\\Zotero\\storage\\PLAYSI3K\\Zhang 等 - 2022 - Modeling Indirect Illumination for Inverse Renderi.pdf;C\:\\Users\\D5\\Zotero\\storage\\834ND4GM\\2204.html}
}

@article{zhangPathspaceDifferentiableRendering2020,
  title = {Path-Space Differentiable Rendering},
  author = {Zhang, Cheng and Miller, Bailey and Yan, Kai and Gkioulekas, Ioannis and Zhao, Shuang},
  date = {2020-08-12},
  journaltitle = {ACM Transactions on Graphics},
  shortjournal = {ACM Trans. Graph.},
  volume = {39},
  number = {4},
  pages = {143:143:1--143:143:19},
  issn = {0730-0301},
  doi = {10.1145/3386569.3392383},
  url = {https://dl.acm.org/doi/10.1145/3386569.3392383},
  urldate = {2023-07-03},
  abstract = {Physics-based differentiable rendering, the estimation of derivatives of radiometric measures with respect to arbitrary scene parameters, has a diverse array of applications from solving analysis-by-synthesis problems to training machine learning pipelines incorporating forward rendering processes. Unfortunately, general-purpose differentiable rendering remains challenging due to the lack of efficient estimators as well as the need to identify and handle complex discontinuities such as visibility boundaries. In this paper, we show how path integrals can be differentiated with respect to arbitrary differentiable changes of a scene. We provide a detailed theoretical analysis of this process and establish new differentiable rendering formulations based on the resulting differential path integrals. Our path-space differentiable rendering formulation allows the design of new Monte Carlo estimators that offer significantly better efficiency than state-of-the-art methods in handling complex geometric discontinuities and light transport phenomena such as caustics. We validate our method by comparing our derivative estimates to those generated using the finite-difference method. To demonstrate the effectiveness of our technique, we compare inverse-rendering performance with a few state-of-the-art differentiable rendering methods.},
  keywords = {differentiable rendering,Monte Carlo rendering,path integral},
  file = {C\:\\Users\\D5\\Zotero\\storage\\2SUUZEKB\\Zhang 等 - 2020 - Path-space differentiable rendering.pdf}
}

@online{zhangPhySGInverseRendering2021,
  title = {{{PhySG}}: {{Inverse Rendering}} with {{Spherical Gaussians}} for {{Physics-based Material Editing}} and {{Relighting}}},
  shorttitle = {{{PhySG}}},
  author = {Zhang, Kai and Luan, Fujun and Wang, Qianqian and Bala, Kavita and Snavely, Noah},
  date = {2021-04-01},
  eprint = {2104.00674},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.00674},
  urldate = {2023-07-04},
  abstract = {We present PhySG, an end-to-end inverse rendering pipeline that includes a fully differentiable renderer and can reconstruct geometry, materials, and illumination from scratch from a set of RGB input images. Our framework represents specular BRDFs and environmental illumination using mixtures of spherical Gaussians, and represents geometry as a signed distance function parameterized as a Multi-Layer Perceptron. The use of spherical Gaussians allows us to efficiently solve for approximate light transport, and our method works on scenes with challenging non-Lambertian reflectance captured under natural, static illumination. We demonstrate, with both synthetic and real data, that our reconstructions not only enable rendering of novel viewpoints, but also physics-based appearance editing of materials and illumination.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\D5\\Zotero\\storage\\XQA8E4IQ\\Zhang 等 - 2021 - PhySG Inverse Rendering with Spherical Gaussians .pdf;C\:\\Users\\D5\\Zotero\\storage\\LPH9UJ5G\\2104.html}
}
