@misc{azinovicNeuralRGBDSurface2022,
  title         = {Neural {{RGB-D Surface Reconstruction}}},
  author        = {Azinovi{\'c}, Dejan and {Martin-Brualla}, Ricardo and Goldman, Dan B. and Nie{\ss}ner, Matthias and Thies, Justus},
  year          = {2022},
  month         = mar,
  number        = {arXiv:2104.04532},
  eprint        = {2104.04532},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2104.04532},
  urldate       = {2023-04-25},
  abstract      = {Obtaining high-quality 3D reconstructions of room-scale scenes is of paramount importance for upcoming applications in AR or VR. These range from mixed reality applications for teleconferencing, virtual measuring, virtual room planing, to robotic applications. While current volume-based view synthesis methods that use neural radiance fields (NeRFs) show promising results in reproducing the appearance of an object or scene, they do not reconstruct an actual surface. The volumetric representation of the surface based on densities leads to artifacts when a surface is extracted using Marching Cubes, since during optimization, densities are accumulated along the ray and are not used at a single sample point in isolation. Instead of this volumetric representation of the surface, we propose to represent the surface using an implicit function (truncated signed distance function). We show how to incorporate this representation in the NeRF framework, and extend it to use depth measurements from a commodity RGB-D sensor, such as a Kinect. In addition, we propose a pose and camera refinement technique which improves the overall reconstruction quality. In contrast to concurrent work on integrating depth priors in NeRF which concentrates on novel view synthesis, our approach is able to reconstruct high-quality, metrical 3D reconstructions.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\3X347KFY\\Azinović 等 - 2022 - Neural RGB-D Surface Reconstruction.pdf;C\:\\Users\\Color\\Zotero\\storage\\WSQ5QE8N\\2104.html}
}

@misc{barronMipNeRF360Unbounded2022,
  title         = {Mip-{{NeRF}} 360: {{Unbounded Anti-Aliased Neural Radiance Fields}}},
  shorttitle    = {Mip-{{NeRF}} 360},
  author        = {Barron, Jonathan T. and Mildenhall, Ben and Verbin, Dor and Srinivasan, Pratul P. and Hedman, Peter},
  year          = {2022},
  month         = mar,
  number        = {arXiv:2111.12077},
  eprint        = {2111.12077},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  urldate       = {2023-04-20},
  abstract      = {Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on "unbounded" scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub "mip-NeRF 360" as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 57\% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\PZAMUU26\\Barron 等 - 2022 - Mip-NeRF 360 Unbounded Anti-Aliased Neural Radian.pdf;C\:\\Users\\Color\\Zotero\\storage\\74HMMP62\\2111.html}
}

@misc{barronMipNeRFMultiscaleRepresentation2021,
  title         = {Mip-{{NeRF}}: {{A Multiscale Representation}} for {{Anti-Aliasing Neural Radiance Fields}}},
  shorttitle    = {Mip-{{NeRF}}},
  author        = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and {Martin-Brualla}, Ricardo and Srinivasan, Pratul P.},
  year          = {2021},
  month         = aug,
  number        = {arXiv:2103.13415},
  eprint        = {2103.13415},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2103.13415},
  urldate       = {2023-04-20},
  abstract      = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call "mip-NeRF" (a la "mipmap"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7\% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17\% on the dataset presented with NeRF and by 60\% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\DJ2UITVV\\Barron 等 - 2021 - Mip-NeRF A Multiscale Representation for Anti-Ali.pdf;C\:\\Users\\Color\\Zotero\\storage\\3IG3962K\\2103.html}
}

@misc{bianNoPeNeRFOptimisingNeural2023,
  title         = {{{NoPe-NeRF}}: {{Optimising Neural Radiance Field}} with {{No Pose Prior}}},
  shorttitle    = {{{NoPe-NeRF}}},
  author        = {Bian, Wenjing and Wang, Zirui and Li, Kejie and Bian, Jia-Wang and Prisacariu, Victor Adrian},
  year          = {2023},
  month         = apr,
  number        = {arXiv:2212.07388},
  eprint        = {2212.07388},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2212.07388},
  urldate       = {2023-04-17},
  abstract      = {Training a Neural Radiance Field (NeRF) without pre-computed camera poses is challenging. Recent advances in this direction demonstrate the possibility of jointly optimising a NeRF and camera poses in forward-facing scenes. However, these methods still face difficulties during dramatic camera movement. We tackle this challenging problem by incorporating undistorted monocular depth priors. These priors are generated by correcting scale and shift parameters during training, with which we are then able to constrain the relative poses between consecutive frames. This constraint is achieved using our proposed novel loss functions. Experiments on real-world indoor and outdoor scenes show that our method can handle challenging camera trajectories and outperforms existing methods in terms of novel view rendering quality and pose estimation accuracy. Our project page is https://nope-nerf.active.vision.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\NSIWF77T\\Bian 等 - 2023 - NoPe-NeRF Optimising Neural Radiance Field with N.pdf;C\:\\Users\\Color\\Zotero\\storage\\H33NF42K\\2212.html}
}

@misc{canessaAltiro3DSceneRepresentation2023,
  title         = {{{altiro3D}}: {{Scene}} Representation from Single Image and Novel View Synthesis},
  shorttitle    = {{{altiro3D}}},
  author        = {Canessa, E. and Tenze, L.},
  year          = {2023},
  month         = apr,
  number        = {arXiv:2304.11161},
  eprint        = {2304.11161},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  urldate       = {2023-04-25},
  abstract      = {We introduce altiro3D, a free extended library developed to represent reality starting from a given original RGB image or flat video. It allows to generate a light-field (or Native) image or video and get a realistic 3D experience. To synthesize N-number of virtual images and add them sequentially into a Quilt collage, we apply MiDaS models for the monocular depth estimation, simple OpenCV and Telea inpainting techniques to map all pixels, and implement a 'Fast' algorithm to handle 3D projection camera and scene transformations along N-viewpoints. We use the degree of depth to move proportionally the pixels, assuming the original image to be at the center of all the viewpoints. altiro3D can also be used with DIBR algorithm to compute intermediate snapshots from a equivalent 'Real (slower)' camera with N-geometric viewpoints, which requires to calibrate a priori several intrinsic and extrinsic camera parameters. We adopt a pixel- and device-based Lookup Table to optimize computing time. The multiple viewpoints and video generated from a single image or frame can be displayed in a free-view LCD display.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Multimedia},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\E8QNAB4L\\Canessa 和 Tenze - 2023 - altiro3D Scene representation from single image a.pdf;C\:\\Users\\Color\\Zotero\\storage\\64JXLWNM\\2304.html}
}

@misc{chenSAMFailsSegment2023,
  title         = {{{SAM Fails}} to {{Segment Anything}}? -- {{SAM-Adapter}}: {{Adapting SAM}} in {{Underperformed Scenes}}: {{Camouflage}}, {{Shadow}}, and {{More}}},
  shorttitle    = {{{SAM Fails}} to {{Segment Anything}}?},
  author        = {Chen, Tianrun and Zhu, Lanyun and Ding, Chaotao and Cao, Runlong and Zhang, Shangzhan and Wang, Yan and Li, Zejian and Sun, Lingyun and Mao, Papa and Zang, Ying},
  year          = {2023},
  month         = apr,
  number        = {arXiv:2304.09148},
  eprint        = {2304.09148},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2304.09148},
  urldate       = {2023-04-19},
  abstract      = {The emergence of large models, also known as foundation models, has brought significant advancements to AI research. One such model is Segment Anything (SAM), which is designed for image segmentation tasks. However, as with other foundation models, our experimental findings suggest that SAM may fail or perform poorly in certain segmentation tasks, such as shadow detection and camouflaged object detection (concealed object detection). This study first paves the way for applying the large pre-trained image segmentation model SAM to these downstream tasks, even in situations where SAM performs poorly. Rather than fine-tuning the SAM network, we propose \textbackslash textbf\{SAM-Adapter\}, which incorporates domain-specific information or visual prompts into the segmentation network by using simple yet effective adapters. Our extensive experiments show that SAM-Adapter can significantly elevate the performance of SAM in challenging tasks and we can even outperform task-specific network models and achieve state-of-the-art performance in the task we tested: camouflaged object detection and shadow detection. We believe our work opens up opportunities for utilizing SAM in downstream tasks, with potential applications in various fields, including medical image processing, agriculture, remote sensing, and more.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\RZ9VMJR9\\Chen 等 - 2023 - SAM Fails to Segment Anything -- SAM-Adapter Ada.pdf;C\:\\Users\\Color\\Zotero\\storage\\CK9KZM65\\2304.html}
}

@misc{chenStructureAwareNeRFPosed2022,
  title         = {Structure-{{Aware NeRF}} without {{Posed Camera}} via {{Epipolar Constraint}}},
  author        = {Chen, Shu and Zhang, Yang and Xu, Yaxin and Zou, Beiji},
  year          = {2022},
  month         = sep,
  number        = {arXiv:2210.00183},
  eprint        = {2210.00183},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2210.00183},
  urldate       = {2023-04-19},
  abstract      = {The neural radiance field (NeRF) for realistic novel view synthesis requires camera poses to be pre-acquired by a structure-from-motion (SfM) approach. This two-stage strategy is not convenient to use and degrades the performance because the error in the pose extraction can propagate to the view synthesis. We integrate the pose extraction and view synthesis into a single end-to-end procedure so they can benefit from each other. For training NeRF models, only RGB images are given, without pre-known camera poses. The camera poses are obtained by the epipolar constraint in which the identical feature in different views has the same world coordinates transformed from the local camera coordinates according to the extracted poses. The epipolar constraint is jointly optimized with pixel color constraint. The poses are represented by a CNN-based deep network, whose input is the related frames. This joint optimization enables NeRF to be aware of the scene's structure that has an improved generalization performance. Extensive experiments on a variety of scenes demonstrate the effectiveness of the proposed approach. Code is available at https://github.com/XTU-PR-LAB/SaNerf.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\NAWUVCPE\\Chen 等 - 2022 - Structure-Aware NeRF without Posed Camera via Epip.pdf;C\:\\Users\\Color\\Zotero\\storage\\SKH45LQL\\2210.html}
}

@misc{dengNeRDiSingleViewNeRF2022,
  title         = {{{NeRDi}}: {{Single-View NeRF Synthesis}} with {{Language-Guided Diffusion}} as {{General Image Priors}}},
  shorttitle    = {{{NeRDi}}},
  author        = {Deng, Congyue and Jiang, Chiyu "Max'' and Qi, Charles R. and Yan, Xinchen and Zhou, Yin and Guibas, Leonidas and Anguelov, Dragomir},
  year          = {2022},
  month         = dec,
  number        = {arXiv:2212.03267},
  eprint        = {2212.03267},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2212.03267},
  urldate       = {2023-04-22},
  abstract      = {2D-to-3D reconstruction is an ill-posed problem, yet humans are good at solving this problem due to their prior knowledge of the 3D world developed over years. Driven by this observation, we propose NeRDi, a single-view NeRF synthesis framework with general image priors from 2D diffusion models. Formulating single-view reconstruction as an image-conditioned 3D generation problem, we optimize the NeRF representations by minimizing a diffusion loss on its arbitrary view renderings with a pretrained image diffusion model under the input-view constraint. We leverage off-the-shelf vision-language models and introduce a two-section language guidance as conditioning inputs to the diffusion model. This is essentially helpful for improving multiview content coherence as it narrows down the general image prior conditioned on the semantic and visual features of the single-view input image. Additionally, we introduce a geometric loss based on estimated depth maps to regularize the underlying 3D geometry of the NeRF. Experimental results on the DTU MVS dataset show that our method can synthesize novel views with higher quality even compared to existing methods trained on this dataset. We also demonstrate our generalizability in zero-shot NeRF synthesis for in-the-wild images.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\43ZN9SEX\\Deng et al. - 2022 - NeRDi Single-View NeRF Synthesis with Language-Gu.pdf;C\:\\Users\\Color\\Zotero\\storage\\ERX7ASJC\\2212.html}
}

@misc{fanUnifiedImplicitNeural2022,
  title         = {Unified {{Implicit Neural Stylization}}},
  author        = {Fan, Zhiwen and Jiang, Yifan and Wang, Peihao and Gong, Xinyu and Xu, Dejia and Wang, Zhangyang},
  year          = {2022},
  month         = aug,
  number        = {arXiv:2204.01943},
  eprint        = {2204.01943},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2204.01943},
  urldate       = {2023-04-17},
  abstract      = {Representing visual signals by implicit representation (e.g., a coordinate based deep network) has prevailed among many vision tasks. This work explores a new intriguing direction: training a stylized implicit representation, using a generalized approach that can apply to various 2D and 3D scenarios. We conduct a pilot study on a variety of implicit functions, including 2D coordinate-based representation, neural radiance field, and signed distance function. Our solution is a Unified Implicit Neural Stylization framework, dubbed INS. In contrary to vanilla implicit representation, INS decouples the ordinary implicit function into a style implicit module and a content implicit module, in order to separately encode the representations from the style image and input scenes. An amalgamation module is then applied to aggregate these information and synthesize the stylized output. To regularize the geometry in 3D scenes, we propose a novel self-distillation geometry consistency loss which preserves the geometry fidelity of the stylized scenes. Comprehensive experiments are conducted on multiple task settings, including novel view synthesis of complex scenes, stylization for implicit surfaces, and fitting images using MLPs. We further demonstrate that the learned representation is continuous not only spatially but also style-wise, leading to effortlessly interpolating between different styles and generating images with new mixed styles. Please refer to the video on our project page for more view synthesis results: https://zhiwenfan.github.io/INS.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\K575GVN6\\Fan 等 - 2022 - Unified Implicit Neural Stylization.pdf;C\:\\Users\\Color\\Zotero\\storage\\YJFW7LYB\\2204.html}
}

@misc{gaoNeRFNeuralRadiance2022,
  title         = {{{NeRF}}: {{Neural Radiance Field}} in {{3D Vision}}, {{A Comprehensive Review}}},
  shorttitle    = {{{NeRF}}},
  author        = {Gao, Kyle and Gao, Yina and He, Hongjie and Lu, Dening and Xu, Linlin and Li, Jonathan},
  year          = {2022},
  month         = dec,
  number        = {arXiv:2210.00379},
  eprint        = {2210.00379},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2210.00379},
  urldate       = {2023-04-25},
  abstract      = {Neural Radiance Field (NeRF), a new novel view synthesis with implicit scene representation has taken the field of Computer Vision by storm. As a novel view synthesis and 3D reconstruction method, NeRF models find applications in robotics, urban mapping, autonomous navigation, virtual reality/augmented reality, and more. Since the original paper by Mildenhall et al., more than 250 preprints were published, with more than 100 eventually being accepted in tier one Computer Vision Conferences. Given NeRF popularity and the current interest in this research area, we believe it necessary to compile a comprehensive survey of NeRF papers from the past two years, which we organized into both architecture, and application based taxonomies. We also provide an introduction to the theory of NeRF based novel view synthesis, and a benchmark comparison of the performance and speed of key NeRF models. By creating this survey, we hope to introduce new researchers to NeRF, provide a helpful reference for influential works in this field, as well as motivate future research directions with our discussion section.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\C8IL7I26\\Gao 等 - 2022 - NeRF Neural Radiance Field in 3D Vision, A Compre.pdf;C\:\\Users\\Color\\Zotero\\storage\\EGQCYBY2\\2210.html}
}

@misc{gardnerDeepParametricIndoor2019,
  title        = {Deep {{Parametric Indoor Lighting Estimation}}},
  author       = {Gardner, Marc-Andr{\'e} and {Hold-Geoffroy}, Yannick and Sunkavalli, Kalyan and Gagn{\'e}, Christian and Lalonde, Jean-Fran{\c c}ois},
  year         = {2019},
  month        = oct,
  journal      = {arXiv.org},
  urldate      = {2023-04-22},
  abstract     = {We present a method to estimate lighting from a single image of an indoor scene. Previous work has used an environment map representation that does not account for the localized nature of indoor lighting. Instead, we represent lighting as a set of discrete 3D lights with geometric and photometric parameters. We train a deep neural network to regress these parameters from a single image, on a dataset of environment maps annotated with depth. We propose a differentiable layer to convert these parameters to an environment map to compute our loss; this bypasses the challenge of establishing correspondences between estimated and ground truth lights. We demonstrate, via quantitative and qualitative evaluations, that our representation and training scheme lead to more accurate results compared to previous work, while allowing for more realistic 3D object compositing with spatially-varying lighting.},
  howpublished = {https://arxiv.org/abs/1910.08812v1},
  langid       = {english},
  file         = {C\:\\Users\\Color\\Zotero\\storage\\QHT4CQLF\\Gardner et al. - 2019 - Deep Parametric Indoor Lighting Estimation.pdf}
}

@inproceedings{gortlerLumigraph1996,
  title     = {The Lumigraph},
  booktitle = {Proceedings of the 23rd Annual Conference on {{Computer}} Graphics and Interactive Techniques},
  author    = {Gortler, Steven J. and Grzeszczuk, Radek and Szeliski, Richard and Cohen, Michael F.},
  year      = {1996},
  month     = aug,
  series    = {{{SIGGRAPH}} '96},
  pages     = {43--54},
  publisher = {{Association for Computing Machinery}},
  address   = {{New York, NY, USA}},
  doi       = {10.1145/237170.237200},
  urldate   = {2023-04-12},
  isbn      = {978-0-89791-746-9},
  file      = {C\:\\Users\\Color\\Zotero\\storage\\J9XE8HJJ\\Gortler 等 - 1996 - The lumigraph.pdf}
}

@misc{guNerfDiffSingleimageView2023,
  title         = {{{NerfDiff}}: {{Single-image View Synthesis}} with {{NeRF-guided Distillation}} from {{3D-aware Diffusion}}},
  shorttitle    = {{{NerfDiff}}},
  author        = {Gu, Jiatao and Trevithick, Alex and Lin, Kai-En and Susskind, Josh and Theobalt, Christian and Liu, Lingjie and Ramamoorthi, Ravi},
  year          = {2023},
  month         = feb,
  number        = {arXiv:2302.10109},
  eprint        = {2302.10109},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2302.10109},
  urldate       = {2023-04-11},
  abstract      = {Novel view synthesis from a single image requires inferring occluded regions of objects and scenes whilst simultaneously maintaining semantic and physical consistency with the input. Existing approaches condition neural radiance fields (NeRF) on local image features, projecting points to the input image plane, and aggregating 2D features to perform volume rendering. However, under severe occlusion, this projection fails to resolve uncertainty, resulting in blurry renderings that lack details. In this work, we propose NerfDiff, which addresses this issue by distilling the knowledge of a 3D-aware conditional diffusion model (CDM) into NeRF through synthesizing and refining a set of virtual views at test time. We further propose a novel NeRF-guided distillation algorithm that simultaneously generates 3D consistent virtual views from the CDM samples, and finetunes the NeRF based on the improved virtual views. Our approach significantly outperforms existing NeRF-based and geometry-free approaches on challenging datasets, including ShapeNet, ABO, and Clevr3D.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\VZ7B98VR\\Gu 等 - 2023 - NerfDiff Single-image View Synthesis with NeRF-gu.pdf;C\:\\Users\\Color\\Zotero\\storage\\G5J4XH84\\2302.html}
}

@misc{huangModelAwareContrastiveLearning2023,
  title         = {Model-{{Aware Contrastive Learning}}: {{Towards Escaping}} the {{Dilemmas}}},
  shorttitle    = {Model-{{Aware Contrastive Learning}}},
  author        = {Huang, Zizheng and Chen, Haoxing and Wen, Ziqi and Zhang, Chao and Li, Huaxiong and Wang, Bo and Chen, Chunlin},
  year          = {2023},
  month         = feb,
  number        = {arXiv:2207.07874},
  eprint        = {2207.07874},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2207.07874},
  urldate       = {2023-04-25},
  abstract      = {Contrastive learning (CL) continuously achieves significant breakthroughs across multiple domains. However, the most common InfoNCE based methods suffer from some existing dilemmas, e.g., uniformity-tolerance dilemma (UTD) and the gradient reduction. It has been identified that UTD can lead to unexpected performance degradation. We argue that the fixity of temperature is to blame for UTD. To tackle this challenge, we enrich the CL loss family by presenting a Model-Aware Contrastive Learning (MACL) strategy, whose temperature is adaptive to the magnitude of alignment that reflects the basic confidence of the instance discrimination task, then enables CL loss to adjust the penalty strength for hard negatives adaptively. Regarding another dilemma, the gradient reduction issue, we derive the limits of an involved gradient scaling factor, which allows us to explain from a unified perspective why some recent approaches are effective with fewer negative samples, and summarily present a gradient reweighting to escape this dilemma. Extensive remarkable empirical results in vision, sentence, and graph modality validate our approach's general improvement for representation learning and downstream tasks.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Machine Learning},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\NN2K3B24\\Huang 等 - 2023 - Model-Aware Contrastive Learning Towards Escaping.pdf;C\:\\Users\\Color\\Zotero\\storage\\65ACWHI6\\2207.html}
}

@article{jambonNeRFshopInteractiveEditing,
  title  = {{{NeRFshop}}: {{Interactive Editing}} of {{Neural Radiance Fields}}},
  author = {Jambon, Cl{\'e}ment},
  volume = {6},
  number = {1},
  langid = {english},
  file   = {C\:\\Users\\Color\\Zotero\\storage\\9JKQ4S94\\Jambon - NeRFshop Interactive Editing of Neural Radiance F.pdf}
}

@inproceedings{kazhdanPoissonSurfaceReconstruction2006,
  title     = {Poisson Surface Reconstruction},
  booktitle = {Proceedings of the Fourth {{Eurographics}} Symposium on {{Geometry}} Processing},
  author    = {Kazhdan, Michael and Bolitho, Matthew and Hoppe, Hugues},
  year      = {2006},
  month     = jun,
  series    = {{{SGP}} '06},
  pages     = {61--70},
  publisher = {{Eurographics Association}},
  address   = {{Goslar, DEU}},
  urldate   = {2023-04-25},
  abstract  = {We show that surface reconstruction from oriented points can be cast as a spatial Poisson problem. This Poisson formulation considers all the points at once, without resorting to heuristic spatial partitioning or blending, and is therefore highly resilient to data noise. Unlike radial basis function schemes, our Poisson approach allows a hierarchy of locally supported basis functions, and therefore the solution reduces to a well conditioned sparse linear system. We describe a spatially adaptive multiscale algorithm whose time and space complexities are proportional to the size of the reconstructed model. Experimenting with publicly available scan data, we demonstrate reconstruction of surfaces with greater detail than previously achievable.},
  isbn      = {978-3-905673-36-4},
  file      = {D\:\\data\\papers\\cv\\3d-reconstruction\\possion-surface-reconstruction.pdf}
}

@article{kazhdanScreenedPoissonSurface2013,
  title    = {Screened Poisson Surface Reconstruction},
  author   = {Kazhdan, Michael and Hoppe, Hugues},
  year     = {2013},
  month    = jul,
  journal  = {ACM Transactions on Graphics},
  volume   = {32},
  number   = {3},
  pages    = {29:1--29:13},
  issn     = {0730-0301},
  doi      = {10.1145/2487228.2487237},
  urldate  = {2023-04-25},
  abstract = {Poisson surface reconstruction creates watertight surfaces from oriented point sets. In this work we extend the technique to explicitly incorporate the points as interpolation constraints. The extension can be interpreted as a generalization of the underlying mathematical framework to a screened Poisson equation. In contrast to other image and geometry processing techniques, the screening term is defined over a sparse set of points rather than over the full domain. We show that these sparse constraints can nonetheless be integrated efficiently. Because the modified linear system retains the same finite-element discretization, the sparsity structure is unchanged, and the system can still be solved using a multigrid approach. Moreover we present several algorithmic improvements that together reduce the time complexity of the solver to linear in the number of points, thereby enabling faster, higher-quality surface reconstructions.},
  keywords = {adaptive octree,finite elements,Screened Poisson equation,surface fitting},
  file     = {C\:\\Users\\Color\\Zotero\\storage\\UANJZQHD\\Kazhdan 和 Hoppe - 2013 - Screened poisson surface reconstruction.pdf}
}

@misc{kulhanekTetraNeRFRepresentingNeural2023,
  title         = {Tetra-{{NeRF}}: {{Representing Neural Radiance Fields Using Tetrahedra}}},
  shorttitle    = {Tetra-{{NeRF}}},
  author        = {Kulhanek, Jonas and Sattler, Torsten},
  year          = {2023},
  month         = apr,
  number        = {arXiv:2304.09987},
  eprint        = {2304.09987},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2304.09987},
  urldate       = {2023-04-25},
  abstract      = {Neural Radiance Fields (NeRFs) are a very recent and very popular approach for the problems of novel view synthesis and 3D reconstruction. A popular scene representation used by NeRFs is to combine a uniform, voxel-based subdivision of the scene with an MLP. Based on the observation that a (sparse) point cloud of the scene is often available, this paper proposes to use an adaptive representation based on tetrahedra and a Delaunay representation instead of the uniform subdivision or point-based representations. We show that such a representation enables efficient training and leads to state-of-the-art results. Our approach elegantly combines concepts from 3D geometry processing, triangle-based rendering, and modern neural radiance fields. Compared to voxel-based representations, ours provides more detail around parts of the scene likely to be close to the surface. Compared to point-based representations, our approach achieves better performance.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\XS9FPK44\\Kulhanek 和 Sattler - 2023 - Tetra-NeRF Representing Neural Radiance Fields Us.pdf;C\:\\Users\\Color\\Zotero\\storage\\T6IHYTQS\\2304.html}
}

@article{le-khacContrastiveRepresentationLearning2020,
  title         = {Contrastive {{Representation Learning}}: {{A Framework}} and {{Review}}},
  shorttitle    = {Contrastive {{Representation Learning}}},
  author        = {{Le-Khac}, Phuc H. and Healy, Graham and Smeaton, Alan F.},
  year          = {2020},
  journal       = {IEEE Access},
  volume        = {8},
  eprint        = {2010.05113},
  primaryclass  = {cs, stat},
  pages         = {193907--193934},
  issn          = {2169-3536},
  doi           = {10.1109/ACCESS.2020.3031549},
  urldate       = {2023-04-25},
  abstract      = {Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\XRK5UCI6\\Le-Khac 等 - 2020 - Contrastive Representation Learning A Framework a.pdf;C\:\\Users\\Color\\Zotero\\storage\\6TNAKZVN\\2010.html}
}

@misc{leeDPNeRFDeblurredNeural2023,
  title         = {{{DP-NeRF}}: {{Deblurred Neural Radiance Field}} with {{Physical Scene Priors}}},
  shorttitle    = {{{DP-NeRF}}},
  author        = {Lee, Dogyoon and Lee, Minhyeok and Shin, Chajin and Lee, Sangyoun},
  year          = {2023},
  month         = mar,
  number        = {arXiv:2211.12046},
  eprint        = {2211.12046},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  urldate       = {2023-04-17},
  abstract      = {Neural Radiance Field (NeRF) has exhibited outstanding three-dimensional (3D) reconstruction quality via the novel view synthesis from multi-view images and paired calibrated camera parameters. However, previous NeRF-based systems have been demonstrated under strictly controlled settings, with little attention paid to less ideal scenarios, including with the presence of noise such as exposure, illumination changes, and blur. In particular, though blur frequently occurs in real situations, NeRF that can handle blurred images has received little attention. The few studies that have investigated NeRF for blurred images have not considered geometric and appearance consistency in 3D space, which is one of the most important factors in 3D reconstruction. This leads to inconsistency and the degradation of the perceptual quality of the constructed scene. Hence, this paper proposes a DP-NeRF, a novel clean NeRF framework for blurred images, which is constrained with two physical priors. These priors are derived from the actual blurring process during image acquisition by the camera. DP-NeRF proposes rigid blurring kernel to impose 3D consistency utilizing the physical priors and adaptive weight proposal to refine the color composition error in consideration of the relationship between depth and blur. We present extensive experimental results for synthetic and real scenes with two types of blur: camera motion blur and defocus blur. The results demonstrate that DP-NeRF successfully improves the perceptual quality of the constructed NeRF ensuring 3D geometric and appearance consistency. We further demonstrate the effectiveness of our model with comprehensive ablation analysis.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\9B34DDDN\\Lee 等 - 2023 - DP-NeRF Deblurred Neural Radiance Field with Phys.pdf;C\:\\Users\\Color\\Zotero\\storage\\4EZRPAY4\\2211.html}
}

@misc{liLift3DSynthesize3D2023,
  title         = {{{Lift3D}}: {{Synthesize 3D Training Data}} by {{Lifting 2D GAN}} to {{3D Generative Radiance Field}}},
  shorttitle    = {{{Lift3D}}},
  author        = {Li, Leheng and Lian, Qing and Wang, Luozhou and Ma, Ningning and Chen, Ying-Cong},
  year          = {2023},
  month         = apr,
  number        = {arXiv:2304.03526},
  eprint        = {2304.03526},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  urldate       = {2023-04-27},
  abstract      = {This work explores the use of 3D generative models to synthesize training data for 3D vision tasks. The key requirements of the generative models are that the generated data should be photorealistic to match the real-world scenarios, and the corresponding 3D attributes should be aligned with given sampling labels. However, we find that the recent NeRF-based 3D GANs hardly meet the above requirements due to their designed generation pipeline and the lack of explicit 3D supervision. In this work, we propose Lift3D, an inverted 2D-to-3D generation framework to achieve the data generation objectives. Lift3D has several merits compared to prior methods: (1) Unlike previous 3D GANs that the output resolution is fixed after training, Lift3D can generalize to any camera intrinsic with higher resolution and photorealistic output. (2) By lifting well-disentangled 2D GAN to 3D object NeRF, Lift3D provides explicit 3D information of generated objects, thus offering accurate 3D annotations for downstream tasks. We evaluate the effectiveness of our framework by augmenting autonomous driving datasets. Experimental results demonstrate that our data generation framework can effectively improve the performance of 3D object detectors. Project page: https://len-li.github.io/lift3d-web.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\AWAAMHKI\\Li 等 - 2023 - Lift3D Synthesize 3D Training Data by Lifting 2D .pdf;C\:\\Users\\Color\\Zotero\\storage\\8M3N294G\\2304.html}
}

@misc{lingShadowNeuSNeuralSDF2023,
  title         = {{{ShadowNeuS}}: {{Neural SDF Reconstruction}} by {{Shadow Ray Supervision}}},
  shorttitle    = {{{ShadowNeuS}}},
  author        = {Ling, Jingwang and Wang, Zhibo and Xu, Feng},
  year          = {2023},
  month         = mar,
  number        = {arXiv:2211.14086},
  eprint        = {2211.14086},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2211.14086},
  urldate       = {2023-04-17},
  abstract      = {By supervising camera rays between a scene and multi-view image planes, NeRF reconstructs a neural scene representation for the task of novel view synthesis. On the other hand, shadow rays between the light source and the scene have yet to be considered. Therefore, we propose a novel shadow ray supervision scheme that optimizes both the samples along the ray and the ray location. By supervising shadow rays, we successfully reconstruct a neural SDF of the scene from single-view images under multiple lighting conditions. Given single-view binary shadows, we train a neural network to reconstruct a complete scene not limited by the camera's line of sight. By further modeling the correlation between the image colors and the shadow rays, our technique can also be effectively extended to RGB inputs. We compare our method with previous works on challenging tasks of shape reconstruction from single-view binary shadow or RGB images and observe significant improvements. The code and data are available at https://github.com/gerwang/ShadowNeuS.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\7IWZ9JJR\\Ling 等 - 2023 - ShadowNeuS Neural SDF Reconstruction by Shadow Ra.pdf;C\:\\Users\\Color\\Zotero\\storage\\Z7DPYG5X\\2211.html}
}

@inproceedings{liPACNeRFPhysicsAugmented2023,
  title      = {{{PAC-NeRF}}: {{Physics Augmented Continuum Neural Radiance Fields}} for {{Geometry-Agnostic System Identification}}},
  shorttitle = {{{PAC-NeRF}}},
  booktitle  = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author     = {Li, Xuan and Qiao, Yi-Ling and Chen, Peter Yichen and Jatavallabhula, Krishna Murthy and Lin, Ming and Jiang, Chenfanfu and Gan, Chuang},
  year       = {2023},
  month      = feb,
  urldate    = {2023-04-17},
  abstract   = {Existing approaches to system identification (estimating the physical parameters of an object) from videos assume known object geometries. This precludes their applicability in a vast majority of scenes where object geometries are complex or unknown. In this work, we aim to identify parameters characterizing a physical system from a set of multi-view videos without any assumption on object geometry or topology. To this end, we propose "Physics Augmented Continuum Neural Radiance Fields" (PAC-NeRF), to estimate both the unknown geometry and physical parameters of highly dynamic objects from multi-view videos. We design PAC-NeRF to only ever produce physically plausible states by enforcing the neural radiance field to follow the conservation laws of continuum mechanics. For this, we design a hybrid Eulerian-Lagrangian representation of the neural radiance field, i.e., we use the Eulerian grid representation for NeRF density and color fields, while advecting the neural radiance fields via Lagrangian particles. This hybrid Eulerian-Lagrangian representation seamlessly blends efficient neural rendering with the material point method (MPM) for robust differentiable physics simulation. We validate the effectiveness of our proposed framework on geometry and physical parameter estimation over a vast range of materials, including elastic bodies, plasticine, sand, Newtonian and non-Newtonian fluids, and demonstrate significant performance gain on most tasks.},
  langid     = {english},
  file       = {C\:\\Users\\Color\\Zotero\\storage\\QUXTVPDX\\Li 等 - 2023 - PAC-NeRF Physics Augmented Continuum Neural Radia.pdf}
}

@misc{liPatchbased3DNatural2023,
  title         = {Patch-Based {{3D Natural Scene Generation}} from a {{Single Example}}},
  author        = {Li, Weiyu and Chen, Xuelin and Wang, Jue and Chen, Baoquan},
  year          = {2023},
  month         = apr,
  number        = {arXiv:2304.12670},
  eprint        = {2304.12670},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2304.12670},
  urldate       = {2023-04-27},
  abstract      = {We target a 3D generative model for general natural scenes that are typically unique and intricate. Lacking the necessary volumes of training data, along with the difficulties of having ad hoc designs in presence of varying scene characteristics, renders existing setups intractable. Inspired by classical patch-based image models, we advocate for synthesizing 3D scenes at the patch level, given a single example. At the core of this work lies important algorithmic designs w.r.t the scene representation and generative patch nearest-neighbor module, that address unique challenges arising from lifting classical 2D patch-based framework to 3D generation. These design choices, on a collective level, contribute to a robust, effective, and efficient model that can generate high-quality general natural scenes with both realistic geometric structure and visual appearance, in large quantities and varieties, as demonstrated upon a variety of exemplar scenes.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\QYVGCU9L\\Li 等 - 2023 - Patch-based 3D Natural Scene Generation from a Sin.pdf;C\:\\Users\\Color\\Zotero\\storage\\8R93WSB7\\2304.html}
}

@misc{liuNeRFInFreeFormNeRF2022,
  title         = {{{NeRF-In}}: {{Free-Form NeRF Inpainting}} with {{RGB-D Priors}}},
  shorttitle    = {{{NeRF-In}}},
  author        = {Liu, Hao-Kang and Shen, I.-Chao and Chen, Bing-Yu},
  year          = {2022},
  month         = jun,
  number        = {arXiv:2206.04901},
  eprint        = {2206.04901},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2206.04901},
  urldate       = {2023-04-17},
  abstract      = {Though Neural Radiance Field (NeRF) demonstrates compelling novel view synthesis results, it is still unintuitive to edit a pre-trained NeRF because the neural network's parameters and the scene geometry/appearance are often not explicitly associated. In this paper, we introduce the first framework that enables users to remove unwanted objects or retouch undesired regions in a 3D scene represented by a pre-trained NeRF without any category-specific data and training. The user first draws a free-form mask to specify a region containing unwanted objects over a rendered view from the pre-trained NeRF. Our framework first transfers the user-provided mask to other rendered views and estimates guiding color and depth images within these transferred masked regions. Next, we formulate an optimization problem that jointly inpaints the image content in all masked regions across multiple views by updating the NeRF model's parameters. We demonstrate our framework on diverse scenes and show it obtained visual plausible and structurally consistent results across multiple views using shorter time and less user manual efforts.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\U7SF3SBA\\Liu 等 - 2022 - NeRF-In Free-Form NeRF Inpainting with RGB-D Prio.pdf;C\:\\Users\\Color\\Zotero\\storage\\6YM8BVKM\\2206.html}
}

@misc{liuNeUDFLeaningNeural2023,
  title         = {{{NeUDF}}: {{Leaning Neural Unsigned Distance Fields}} with {{Volume Rendering}}},
  shorttitle    = {{{NeUDF}}},
  author        = {Liu, Yu-Tao and Wang, Li and {yang}, Jie and Chen, Weikai and Meng, Xiaoxu and Yang, Bo and Gao, Lin},
  year          = {2023},
  month         = apr,
  number        = {arXiv:2304.10080},
  eprint        = {2304.10080},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2304.10080},
  urldate       = {2023-04-25},
  abstract      = {Multi-view shape reconstruction has achieved impressive progresses thanks to the latest advances in neural implicit surface rendering. However, existing methods based on signed distance function (SDF) are limited to closed surfaces, failing to reconstruct a wide range of real-world objects that contain open-surface structures. In this work, we introduce a new neural rendering framework, coded NeUDF, that can reconstruct surfaces with arbitrary topologies solely from multi-view supervision. To gain the flexibility of representing arbitrary surfaces, NeUDF leverages the unsigned distance function (UDF) as surface representation. While a naive extension of an SDF-based neural renderer cannot scale to UDF, we propose two new formulations of weight function specially tailored for UDF-based volume rendering. Furthermore, to cope with open surface rendering, where the in/out test is no longer valid, we present a dedicated normal regularization strategy to resolve the surface orientation ambiguity. We extensively evaluate our method over a number of challenging datasets, including DTU\vphantom\{\}, MGN, and Deep Fashion 3D. Experimental results demonstrate that nEudf can significantly outperform the state-of-the-art method in the task of multi-view surface reconstruction, especially for complex shapes with open boundaries.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\XNL9W9XQ\\Liu 等 - 2023 - NeUDF Leaning Neural Unsigned Distance Fields wit.pdf;C\:\\Users\\Color\\Zotero\\storage\\XRUIGYZV\\2304.html}
}

@misc{liuRobustDynamicRadiance2023,
  title         = {Robust {{Dynamic Radiance Fields}}},
  author        = {Liu, Yu-Lun and Gao, Chen and Meuleman, Andreas and Tseng, Hung-Yu and Saraf, Ayush and Kim, Changil and Chuang, Yung-Yu and Kopf, Johannes and Huang, Jia-Bin},
  year          = {2023},
  month         = mar,
  number        = {arXiv:2301.02239},
  eprint        = {2301.02239},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2301.02239},
  urldate       = {2023-04-17},
  abstract      = {Dynamic radiance field reconstruction methods aim to model the time-varying structure and appearance of a dynamic scene. Existing methods, however, assume that accurate camera poses can be reliably estimated by Structure from Motion (SfM) algorithms. These methods, thus, are unreliable as SfM algorithms often fail or produce erroneous poses on challenging videos with highly dynamic objects, poorly textured surfaces, and rotating camera motion. We address this robustness issue by jointly estimating the static and dynamic radiance fields along with the camera parameters (poses and focal length). We demonstrate the robustness of our approach via extensive quantitative and qualitative experiments. Our results show favorable performance over the state-of-the-art dynamic view synthesis methods.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\EUB92CIJ\\Liu 等 - 2023 - Robust Dynamic Radiance Fields.pdf;C\:\\Users\\Color\\Zotero\\storage\\UL6A6WJB\\2301.html}
}

@misc{liuZero1to3ZeroshotOne2023,
  title         = {Zero-1-to-3: {{Zero-shot One Image}} to {{3D Object}}},
  shorttitle    = {Zero-1-to-3},
  author        = {Liu, Ruoshi and Wu, Rundi and Van Hoorick, Basile and Tokmakov, Pavel and Zakharov, Sergey and Vondrick, Carl},
  year          = {2023},
  month         = mar,
  number        = {arXiv:2303.11328},
  eprint        = {2303.11328},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2303.11328},
  urldate       = {2023-04-17},
  abstract      = {We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an object given just a single RGB image. To perform novel view synthesis in this under-constrained setting, we capitalize on the geometric priors that large-scale diffusion models learn about natural images. Our conditional diffusion model uses a synthetic dataset to learn controls of the relative camera viewpoint, which allow new images to be generated of the same object under a specified camera transformation. Even though it is trained on a synthetic dataset, our model retains a strong zero-shot generalization ability to out-of-distribution datasets as well as in-the-wild images, including impressionist paintings. Our viewpoint-conditioned diffusion approach can further be used for the task of 3D reconstruction from a single image. Qualitative and quantitative experiments show that our method significantly outperforms state-of-the-art single-view 3D reconstruction and novel view synthesis models by leveraging Internet-scale pre-training.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Robotics},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\77Y28R62\\Liu 等 - 2023 - Zero-1-to-3 Zero-shot One Image to 3D Object.pdf;C\:\\Users\\Color\\Zotero\\storage\\QY2X3TSF\\2303.html}
}

@misc{martin-bruallaNeRFWildNeural2021,
  title         = {{{NeRF}} in the {{Wild}}: {{Neural Radiance Fields}} for {{Unconstrained Photo Collections}}},
  shorttitle    = {{{NeRF}} in the {{Wild}}},
  author        = {{Martin-Brualla}, Ricardo and Radwan, Noha and Sajjadi, Mehdi S. M. and Barron, Jonathan T. and Dosovitskiy, Alexey and Duckworth, Daniel},
  year          = {2021},
  month         = jan,
  number        = {arXiv:2008.02268},
  eprint        = {2008.02268},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2008.02268},
  urldate       = {2023-04-27},
  abstract      = {We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\Q5CKNP75\\Martin-Brualla 等 - 2021 - NeRF in the Wild Neural Radiance Fields for Uncon.pdf;C\:\\Users\\Color\\Zotero\\storage\\6MW2VAFC\\2008.html}
}

@misc{mildenhallLocalLightField2019,
  title         = {Local {{Light Field Fusion}}: {{Practical View Synthesis}} with {{Prescriptive Sampling Guidelines}}},
  shorttitle    = {Local {{Light Field Fusion}}},
  author        = {Mildenhall, Ben and Srinivasan, Pratul P. and {Ortiz-Cayon}, Rodrigo and Kalantari, Nima Khademi and Ramamoorthi, Ravi and Ng, Ren and Kar, Abhishek},
  year          = {2019},
  month         = may,
  number        = {arXiv:1905.00889},
  eprint        = {1905.00889},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.1905.00889},
  urldate       = {2023-03-06},
  abstract      = {We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000x fewer views. We demonstrate our approach's practicality with an augmented reality smartphone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\SDR2GUYE\\Mildenhall et al. - 2019 - Local Light Field Fusion Practical View Synthesis.pdf;C\:\\Users\\Color\\Zotero\\storage\\N8DEHSVB\\1905.html}
}

@inproceedings{mildenhallNeRFRepresentingScenes2020,
  title      = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  booktitle  = {{{ECCV}}},
  author     = {Mildenhall, B. and Srinivasan, P. and Tancik, Matthew and Barron, J. and Ramamoorthi, R. and Ng, Ren},
  year       = {2020},
  doi        = {10.1007/978-3-030-58452-8_24},
  abstract   = {This work describes how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrates results that outperform prior work on neural rendering and view synthesis. We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(\textbackslash theta, \textbackslash phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  file       = {C\:\\Users\\Color\\Zotero\\storage\\83S76ZFX\\Mildenhall 等。 - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf}
}

@misc{mirzaeiSPInNeRFMultiviewSegmentation2023,
  title         = {{{SPIn-NeRF}}: {{Multiview Segmentation}} and {{Perceptual Inpainting}} with {{Neural Radiance Fields}}},
  shorttitle    = {{{SPIn-NeRF}}},
  author        = {Mirzaei, Ashkan and {Aumentado-Armstrong}, Tristan and Derpanis, Konstantinos G. and Kelly, Jonathan and Brubaker, Marcus A. and Gilitschenski, Igor and Levinshtein, Alex},
  year          = {2023},
  month         = mar,
  number        = {arXiv:2211.12254},
  eprint        = {2211.12254},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2211.12254},
  urldate       = {2023-04-18},
  abstract      = {Neural Radiance Fields (NeRFs) have emerged as a popular approach for novel view synthesis. While NeRFs are quickly being adapted for a wider set of applications, intuitively editing NeRF scenes is still an open challenge. One important editing task is the removal of unwanted objects from a 3D scene, such that the replaced region is visually plausible and consistent with its context. We refer to this task as 3D inpainting. In 3D, solutions must be both consistent across multiple views and geometrically valid. In this paper, we propose a novel 3D inpainting method that addresses these challenges. Given a small set of posed images and sparse annotations in a single input image, our framework first rapidly obtains a 3D segmentation mask for a target object. Using the mask, a perceptual optimizationbased approach is then introduced that leverages learned 2D image inpainters, distilling their information into 3D space, while ensuring view consistency. We also address the lack of a diverse benchmark for evaluating 3D scene inpainting methods by introducing a dataset comprised of challenging real-world scenes. In particular, our dataset contains views of the same scene with and without a target object, enabling more principled benchmarking of the 3D inpainting task. We first demonstrate the superiority of our approach on multiview segmentation, comparing to NeRFbased methods and 2D segmentation approaches. We then evaluate on the task of 3D inpainting, establishing state-ofthe-art performance against other NeRF manipulation algorithms, as well as a strong 2D image inpainter baseline. Project Page: https://spinnerf3d.github.io},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\EF4GV2PD\\Mirzaei et al. - 2023 - SPIn-NeRF Multiview Segmentation and Perceptual I.pdf;C\:\\Users\\Color\\Zotero\\storage\\58FQR2VG\\2211.html}
}

@misc{mittalNeuralRadianceFields2023,
  title        = {Neural {{Radiance Fields}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle   = {Neural {{Radiance Fields}}},
  author       = {Mittal, Ansh},
  year         = {2023},
  month        = apr,
  journal      = {arXiv.org},
  urldate      = {2023-04-28},
  abstract     = {The various aspects like modeling and interpreting 3D environments and surroundings have enticed humans to progress their research in 3D Computer Vision, Computer Graphics, and Machine Learning. An attempt made by Mildenhall et al in their paper about NeRFs (Neural Radiance Fields) led to a boom in Computer Graphics, Robotics, Computer Vision, and the possible scope of High-Resolution Low Storage Augmented Reality and Virtual Reality-based 3D models have gained traction from res with more than 500 preprints related to NeRFs published. This paper serves as a bridge for people starting to study these fields by building on the basics of Mathematics, Geometry, Computer Vision, and Computer Graphics to the difficulties encountered in Implicit Representations at the intersection of all these disciplines. This survey provides the history of rendering, Implicit Learning, and NeRFs, the progression of research on NeRFs, and the potential applications and implications of NeRFs in today's world. In doing so, this survey categorizes all the NeRF-related research in terms of the datasets used, objective functions, applications solved, and evaluation criteria for these applications.},
  howpublished = {https://arxiv.org/abs/2304.10050v1},
  langid       = {english},
  file         = {C\:\\Users\\Color\\Zotero\\storage\\SCUTXTPA\\Mittal - 2023 - Neural Radiance Fields Past, Present, and Future.pdf}
}

@article{mullerInstantNeuralGraphics2022,
  title         = {Instant {{Neural Graphics Primitives}} with a {{Multiresolution Hash Encoding}}},
  author        = {M{\"u}ller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
  year          = {2022},
  month         = jul,
  journal       = {ACM Transactions on Graphics},
  volume        = {41},
  number        = {4},
  eprint        = {2201.05989},
  primaryclass  = {cs},
  pages         = {1--15},
  issn          = {0730-0301, 1557-7368},
  doi           = {10.1145/3528223.3530127},
  urldate       = {2023-04-20},
  abstract      = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of \$\{1920\textbackslash!\textbackslash times\textbackslash!1080\}\$.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\T4TWTCUV\\Müller 等 - 2022 - Instant Neural Graphics Primitives with a Multires.pdf;C\:\\Users\\Color\\Zotero\\storage\\ERVZ6HRM\\2201.html}
}

@inproceedings{mullerInstantNeuralRadiance2022,
  title     = {Instant {{Neural Radiance Fields}}},
  booktitle = {{{ACM SIGGRAPH}} 2022 {{Real-Time Live}}!},
  author    = {M{\"u}ller, Thomas and Evans, Alex and Schied, Christoph and Foco, Marco and {B{\'o}dis-Szomor{\'u}}, Andr{\'a}s and Deutsch, Isaac and Shelley, Michael and Keller, Alexander},
  year      = {2022},
  month     = jul,
  series    = {{{SIGGRAPH}} '22},
  pages     = {1--2},
  publisher = {{Association for Computing Machinery}},
  address   = {{New York, NY, USA}},
  doi       = {10.1145/3532833.3538678},
  urldate   = {2023-04-19},
  abstract  = {We extend our instant NeRF implementation [M\"uller et al. 2022] to allow training from an incremental stream of images and camera poses, provided by a realtime Simultaneous Localization And Mapping (SLAM) system. Camera poses are refined end-to-end by back-propagating the gradients from NeRF training. Reconstruction quality is further improved by compensating for various camera properties, such as rolling shutter, non-linear lens distortion, and variable exposure typical of digital cameras. Static scenes can be scanned, the NeRF model trained, and the reconstruction verified in an interactive fashion, in under a minute.},
  isbn      = {978-1-4503-9368-3},
  file      = {C\:\\Users\\Color\\Zotero\\storage\\FWCSL78S\\Müller 等 - 2022 - Instant Neural Radiance Fields.pdf}
}

@article{mullerRealtimeNeuralRadiance2021,
  title         = {Real-Time {{Neural Radiance Caching}} for {{Path Tracing}}},
  author        = {M{\"u}ller, Thomas and Rousselle, Fabrice and Nov{\'a}k, Jan and Keller, Alexander},
  year          = {2021},
  month         = aug,
  journal       = {ACM Transactions on Graphics},
  volume        = {40},
  number        = {4},
  eprint        = {2106.12372},
  primaryclass  = {cs},
  pages         = {1--16},
  issn          = {0730-0301, 1557-7368},
  doi           = {10.1145/3450626.3459812},
  urldate       = {2023-04-20},
  abstract      = {We present a real-time neural radiance caching method for path-traced global illumination. Our system is designed to handle fully dynamic scenes, and makes no assumptions about the lighting, geometry, and materials. The data-driven nature of our approach sidesteps many difficulties of caching algorithms, such as locating, interpolating, and updating cache points. Since pretraining neural networks to handle novel, dynamic scenes is a formidable generalization challenge, we do away with pretraining and instead achieve generalization via adaptation, i.e. we opt for training the radiance cache while rendering. We employ self-training to provide low-noise training targets and simulate infinite-bounce transport by merely iterating few-bounce training updates. The updates and cache queries incur a mild overhead -- about 2.6ms on full HD resolution -- thanks to a streaming implementation of the neural network that fully exploits modern hardware. We demonstrate significant noise reduction at the cost of little induced bias, and report state-of-the-art, real-time performance on a number of challenging scenarios.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Graphics,Computer Science - Machine Learning},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\9C6XCJ9K\\Müller 等 - 2021 - Real-time Neural Radiance Caching for Path Tracing.pdf;C\:\\Users\\Color\\Zotero\\storage\\FGVK7HYS\\2106.html}
}

@misc{nicholPointESystemGenerating2022,
  title         = {Point-{{E}}: {{A System}} for {{Generating 3D Point Clouds}} from {{Complex Prompts}}},
  shorttitle    = {Point-{{E}}},
  author        = {Nichol, Alex and Jun, Heewoo and Dhariwal, Prafulla and Mishkin, Pamela and Chen, Mark},
  year          = {2022},
  month         = dec,
  number        = {arXiv:2212.08751},
  eprint        = {2212.08751},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2212.08751},
  urldate       = {2023-04-22},
  abstract      = {While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\T36FPJPA\\Nichol et al. - 2022 - Point-E A System for Generating 3D Point Clouds f.pdf;C\:\\Users\\Color\\Zotero\\storage\\4E29A66U\\2212.html}
}

@misc{niemeyerGIRAFFERepresentingScenes2021,
  title         = {{{GIRAFFE}}: {{Representing Scenes}} as {{Compositional Generative Neural Feature Fields}}},
  shorttitle    = {{{GIRAFFE}}},
  author        = {Niemeyer, Michael and Geiger, Andreas},
  year          = {2021},
  month         = apr,
  number        = {arXiv:2011.12100},
  eprint        = {2011.12100},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2011.12100},
  urldate       = {2023-04-27},
  abstract      = {Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\8YV9I63M\\Niemeyer 和 Geiger - 2021 - GIRAFFE Representing Scenes as Compositional Gene.pdf;C\:\\Users\\Color\\Zotero\\storage\\DSTHVB9D\\2011.html}
}

@misc{oechsleUNISURFUnifyingNeural2021,
  title         = {{{UNISURF}}: {{Unifying Neural Implicit Surfaces}} and {{Radiance Fields}} for {{Multi-View Reconstruction}}},
  shorttitle    = {{{UNISURF}}},
  author        = {Oechsle, Michael and Peng, Songyou and Geiger, Andreas},
  year          = {2021},
  month         = oct,
  number        = {arXiv:2104.10078},
  eprint        = {2104.10078},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2104.10078},
  urldate       = {2023-04-25},
  abstract      = {Neural implicit 3D representations have emerged as a powerful paradigm for reconstructing surfaces from multi-view images and synthesizing novel views. Unfortunately, existing methods such as DVR or IDR require accurate per-pixel object masks as supervision. At the same time, neural radiance fields have revolutionized novel view synthesis. However, NeRF's estimated volume density does not admit accurate surface reconstruction. Our key insight is that implicit surface models and radiance fields can be formulated in a unified way, enabling both surface and volume rendering using the same model. This unified perspective enables novel, more efficient sampling procedures and the ability to reconstruct accurate surfaces without input masks. We compare our method on the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments demonstrate that we outperform NeRF in terms of reconstruction quality while performing on par with IDR without requiring masks.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\WKVT9VUL\\Oechsle 等 - 2021 - UNISURF Unifying Neural Implicit Surfaces and Rad.pdf;C\:\\Users\\Color\\Zotero\\storage\\TGUFUG6T\\2104.html}
}

@misc{parkGenerativeAgentsInteractive2023,
  title         = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle    = {Generative {{Agents}}},
  author        = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  year          = {2023},
  month         = apr,
  number        = {arXiv:2304.03442},
  eprint        = {2304.03442},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  urldate       = {2023-04-25},
  abstract      = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\NXBF4WV7\\Park 等 - 2023 - Generative Agents Interactive Simulacra of Human .pdf;C\:\\Users\\Color\\Zotero\\storage\\I9WNM3FC\\2304.html}
}

@inproceedings{pumarolaDNeRFNeuralRadiance2021,
  title      = {D-{{NeRF}}: {{Neural Radiance Fields}} for {{Dynamic Scenes}}},
  shorttitle = {D-{{NeRF}}},
  booktitle  = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author     = {Pumarola, Albert and Corona, Enric and {Pons-Moll}, Gerard and {Moreno-Noguer}, Francesc},
  year       = {2021},
  month      = jun,
  pages      = {10313--10322},
  publisher  = {{IEEE}},
  address    = {{Nashville, TN, USA}},
  doi        = {10.1109/CVPR46437.2021.01018},
  urldate    = {2022-12-27},
  isbn       = {978-1-66544-509-2},
  langid     = {english},
  file       = {E\:\\data\\papers\\DNeRF.pdf}
}

@misc{reiserMERFMemoryEfficientRadiance2023,
  title         = {{{MERF}}: {{Memory-Efficient Radiance Fields}} for {{Real-time View Synthesis}} in {{Unbounded Scenes}}},
  shorttitle    = {{{MERF}}},
  author        = {Reiser, Christian and Szeliski, Richard and Verbin, Dor and Srinivasan, Pratul P. and Mildenhall, Ben and Geiger, Andreas and Barron, Jonathan T. and Hedman, Peter},
  year          = {2023},
  month         = feb,
  number        = {arXiv:2302.12249},
  eprint        = {2302.12249},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2302.12249},
  urldate       = {2023-04-11},
  abstract      = {Neural radiance fields enable state-of-the-art photorealistic view synthesis. However, existing radiance field representations are either too compute-intensive for real-time rendering or require too much memory to scale to large scenes. We present a Memory-Efficient Radiance Field (MERF) representation that achieves real-time rendering of large-scale scenes in a browser. MERF reduces the memory consumption of prior sparse volumetric radiance fields using a combination of a sparse feature grid and high-resolution 2D feature planes. To support large-scale unbounded scenes, we introduce a novel contraction function that maps scene coordinates into a bounded volume while still allowing for efficient ray-box intersection. We design a lossless procedure for baking the parameterization used during training into a model that achieves real-time rendering while still preserving the photorealistic view synthesis quality of a volumetric radiance field.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\BLHA28D9\\Reiser 等 - 2023 - MERF Memory-Efficient Radiance Fields for Real-ti.pdf;C\:\\Users\\Color\\Zotero\\storage\\JWGY6X8M\\2302.html}
}

@misc{shenAnything3DSingleviewAnything2023,
  title         = {Anything-{{3D}}: {{Towards Single-view Anything Reconstruction}} in the {{Wild}}},
  shorttitle    = {Anything-{{3D}}},
  author        = {Shen, Qiuhong and Yang, Xingyi and Wang, Xinchao},
  year          = {2023},
  month         = apr,
  number        = {arXiv:2304.10261},
  eprint        = {2304.10261},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2304.10261},
  urldate       = {2023-04-25},
  abstract      = {3D reconstruction from a single-RGB image in unconstrained real-world scenarios presents numerous challenges due to the inherent diversity and complexity of objects and environments. In this paper, we introduce Anything-3D, a methodical framework that ingeniously combines a series of visual-language models and the Segment-Anything object segmentation model to elevate objects to 3D, yielding a reliable and versatile system for single-view conditioned 3D reconstruction task. Our approach employs a BLIP model to generate textural descriptions, utilizes the Segment-Anything model for the effective extraction of objects of interest, and leverages a text-to-image diffusion model to lift object into a neural radiance field. Demonstrating its ability to produce accurate and detailed 3D reconstructions for a wide array of objects, \textbackslash emph\{Anything-3D\textbackslash footnotemark[2]\} shows promise in addressing the limitations of existing methodologies. Through comprehensive experiments and evaluations on various datasets, we showcase the merits of our approach, underscoring its potential to contribute meaningfully to the field of 3D reconstruction. Demos and code will be available at \textbackslash href\{https://github.com/Anything-of-anything/Anything-3D\}\{https://github.com/Anything-of-anything/Anything-3D\}.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\297AVDK2\\Shen 等 - 2023 - Anything-3D Towards Single-view Anything Reconstr.pdf;C\:\\Users\\Color\\Zotero\\storage\\UCKDWG5G\\2304.html}
}

@misc{songConsistencyModels2023,
  title         = {Consistency {{Models}}},
  author        = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
  year          = {2023},
  month         = mar,
  number        = {arXiv:2303.01469},
  eprint        = {2303.01469},
  primaryclass  = {cs, stat},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2303.01469},
  urldate       = {2023-04-27},
  abstract      = {Diffusion models have made significant breakthroughs in image, audio, and video generation, but they depend on an iterative generation process that causes slow sampling speed and caps their potential for real-time applications. To overcome this limitation, we propose consistency models, a new family of generative models that achieve high sample quality without adversarial training. They support fast one-step generation by design, while still allowing for few-step sampling to trade compute for sample quality. They also support zero-shot data editing, like image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either as a way to distill pre-trained diffusion models, or as standalone generative models. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step generation. For example, we achieve the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained as standalone generative models, consistency models also outperform single-step, non-adversarial generative models on standard benchmarks like CIFAR-10, ImageNet 64x64 and LSUN 256x256.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\HKIFZNMU\\Song 等 - 2023 - Consistency Models.pdf;C\:\\Users\\Color\\Zotero\\storage\\GXGHWLCU\\2303.html}
}

@misc{sunDirectVoxelGrid2022,
  title         = {Direct {{Voxel Grid Optimization}}: {{Super-fast Convergence}} for {{Radiance Fields Reconstruction}}},
  shorttitle    = {Direct {{Voxel Grid Optimization}}},
  author        = {Sun, Cheng and Sun, Min and Chen, Hwann-Tzong},
  year          = {2022},
  month         = jun,
  number        = {arXiv:2111.11215},
  eprint        = {2111.11215},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2111.11215},
  urldate       = {2023-04-17},
  abstract      = {We present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolutionized by Neural Radiance Field (NeRF) for its state-of-the-art quality and flexibility. However, NeRF and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, NeRF's quality, yet it only takes about 15 minutes to train from scratch for a new scene.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\W6VX8U39\\Sun 等 - 2022 - Direct Voxel Grid Optimization Super-fast Converg.pdf;C\:\\Users\\Color\\Zotero\\storage\\3NI3PFKN\\2111.html}
}

@misc{wangCLIPNeRFTextandImageDriven2022,
  title         = {{{CLIP-NeRF}}: {{Text-and-Image Driven Manipulation}} of {{Neural Radiance Fields}}},
  shorttitle    = {{{CLIP-NeRF}}},
  author        = {Wang, Can and Chai, Menglei and He, Mingming and Chen, Dongdong and Liao, Jing},
  year          = {2022},
  month         = mar,
  number        = {arXiv:2112.05139},
  eprint        = {2112.05139},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2112.05139},
  urldate       = {2023-04-27},
  abstract      = {We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural radiance fields (NeRF). By leveraging the joint language-image embedding space of the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose a unified framework that allows manipulating NeRF in a user-friendly way, using either a short text prompt or an exemplar image. Specifically, to combine the novel view synthesis capability of NeRF and the controllable manipulation ability of latent representations from generative models, we introduce a disentangled conditional NeRF architecture that allows individual control over both shape and appearance. This is achieved by performing the shape conditioning via applying a learned deformation field to the positional encoding and deferring color conditioning to the volumetric rendering stage. To bridge this disentangled latent representation to the CLIP embedding, we design two code mappers that take a CLIP embedding as input and update the latent codes to reflect the targeted editing. The mappers are trained with a CLIP-based matching loss to ensure the manipulation accuracy. Furthermore, we propose an inverse optimization method that accurately projects an input image to the latent codes for manipulation to enable editing on real images. We evaluate our approach by extensive experiments on a variety of text prompts and exemplar images and also provide an intuitive interface for interactive editing. Our implementation is available at https://cassiepython.github.io/clipnerf/},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\DQVCDXS7\\Wang 等 - 2022 - CLIP-NeRF Text-and-Image Driven Manipulation of N.pdf;C\:\\Users\\Color\\Zotero\\storage\\MQKR9LEL\\2112.html}
}

@misc{wangFourierPlenOctreesDynamic2022,
  title         = {Fourier {{PlenOctrees}} for {{Dynamic Radiance Field Rendering}} in {{Real-time}}},
  author        = {Wang, Liao and Zhang, Jiakai and Liu, Xinhang and Zhao, Fuqiang and Zhang, Yanshun and Zhang, Yingliang and Wu, Minye and Xu, Lan and Yu, Jingyi},
  year          = {2022},
  month         = feb,
  number        = {arXiv:2202.08614},
  eprint        = {2202.08614},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2202.08614},
  urldate       = {2023-04-19},
  abstract      = {Implicit neural representations such as Neural Radiance Field (NeRF) have focused mainly on modeling static objects captured under multi-view settings where real-time rendering can be achieved with smart data structures, e.g., PlenOctree. In this paper, we present a novel Fourier PlenOctree (FPO) technique to tackle efficient neural modeling and real-time rendering of dynamic scenes captured under the free-view video (FVV) setting. The key idea in our FPO is a novel combination of generalized NeRF, PlenOctree representation, volumetric fusion and Fourier transform. To accelerate FPO construction, we present a novel coarse-to-fine fusion scheme that leverages the generalizable NeRF technique to generate the tree via spatial blending. To tackle dynamic scenes, we tailor the implicit network to model the Fourier coefficients of timevarying density and color attributes. Finally, we construct the FPO and train the Fourier coefficients directly on the leaves of a union PlenOctree structure of the dynamic sequence. We show that the resulting FPO enables compact memory overload to handle dynamic objects and supports efficient fine-tuning. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF and achieves over an order of magnitude acceleration over SOTA while preserving high visual quality for the free-viewpoint rendering of unseen dynamic scenes.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\QC5CMJFA\\Wang 等 - 2022 - Fourier PlenOctrees for Dynamic Radiance Field Ren.pdf;C\:\\Users\\Color\\Zotero\\storage\\3VGB59E4\\2202.html}
}

@article{wederRemovingObjectsNeural,
  title    = {Removing {{Objects From Neural Radiance Fields}}},
  author   = {Weder, Silvan and {Garcia-Hernando}, Guillermo and Brostow, Gabriel and Firman, Michael and Vicente, Sara},
  abstract = {Neural Radiance Fields (NeRFs) are emerging as a ubiquitous scene representation that allows for novel view synthesis. Increasingly, NeRFs will be shareable with other people. Before sharing a NeRF, though, it might be desirable to remove personal information or unsightly objects. Such removal is not easily achieved with the current NeRF editing frameworks. We propose a framework to remove OuoDbrsjseecqtsuefnrocme.},
  langid   = {english},
  file     = {C\:\\Users\\Color\\Zotero\\storage\\5358ZX5G\\Weder 等 - Removing Objects From Neural Radiance Fields.pdf}
}

@misc{wuPaletteNeRFPalettebasedColor2022,
  title         = {{{PaletteNeRF}}: {{Palette-based Color Editing}} for {{NeRFs}}},
  shorttitle    = {{{PaletteNeRF}}},
  author        = {Wu, Qiling and Tan, Jianchao and Xu, Kun},
  year          = {2022},
  month         = dec,
  number        = {arXiv:2212.12871},
  eprint        = {2212.12871},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2212.12871},
  urldate       = {2022-12-30},
  abstract      = {Neural Radiance Field (NeRF) is a powerful tool to faithfully generate novel views for scenes with only sparse captured images. Despite its strong capability for representing 3D scenes and their appearance, its editing ability is very limited. In this paper, we propose a simple but effective extension of vanilla NeRF, named PaletteNeRF, to enable efficient color editing on NeRF-represented scenes. Motivated by recent palette-based image decomposition works, we approximate each pixel color as a sum of palette colors modulated by additive weights. Instead of predicting pixel colors as in vanilla NeRFs, our method predicts additive weights. The underlying NeRF backbone could also be replaced with more recent NeRF models such as KiloNeRF to achieve real-time editing. Experimental results demonstrate that our method achieves efficient, view-consistent, and artifact-free color editing on a wide range of NeRF-represented scenes.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\8RWYRD53\\Wu 等 - 2022 - PaletteNeRF Palette-based Color Editing for NeRFs.pdf;C\:\\Users\\Color\\Zotero\\storage\\4K9R6VRS\\2212.html}
}

@misc{xueGIRAFFEHDHighResolution2022,
  title        = {{{GIRAFFE HD}}: {{A High-Resolution 3D-aware Generative Model}}},
  shorttitle   = {{{GIRAFFE HD}}},
  author       = {Xue, Yang and Li, Yuheng and Singh, Krishna Kumar and Lee, Yong Jae},
  year         = {2022},
  month        = mar,
  journal      = {arXiv.org},
  urldate      = {2023-04-27},
  abstract     = {3D-aware generative models have shown that the introduction of 3D information can lead to more controllable image generation. In particular, the current state-of-the-art model GIRAFFE can control each object's rotation, translation, scale, and scene camera pose without corresponding supervision. However, GIRAFFE only operates well when the image resolution is low. We propose GIRAFFE HD, a high-resolution 3D-aware generative model that inherits all of GIRAFFE's controllable features while generating high-quality, high-resolution images (\$512\^2\$ resolution and above). The key idea is to leverage a style-based neural renderer, and to independently generate the foreground and background to force their disentanglement while imposing consistency constraints to stitch them together to composite a coherent final image. We demonstrate state-of-the-art 3D controllable high-resolution image generation on multiple natural image datasets.},
  howpublished = {https://arxiv.org/abs/2203.14954v1},
  langid       = {english},
  file         = {C\:\\Users\\Color\\Zotero\\storage\\Q6IRUH8L\\Xue 等 - 2022 - GIRAFFE HD A High-Resolution 3D-aware Generative .pdf}
}

@misc{xuGloballyConsistentNormal2023,
  title         = {Globally {{Consistent Normal Orientation}} for {{Point Clouds}} by {{Regularizing}} the {{Winding-Number Field}}},
  author        = {Xu, Rui and Dou, Zhiyang and Wang, Ningna and Xin, Shiqing and Chen, Shuangmin and Jiang, Mingyan and Guo, Xiaohu and Wang, Wenping and Tu, Changhe},
  year          = {2023},
  month         = apr,
  number        = {arXiv:2304.11605},
  eprint        = {2304.11605},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  urldate       = {2023-04-25},
  abstract      = {Estimating normals with globally consistent orientations for a raw point cloud has many downstream geometry processing applications. Despite tremendous efforts in the past decades, it remains challenging to deal with an unoriented point cloud with various imperfections, particularly in the presence of data sparsity coupled with nearby gaps or thin-walled structures. In this paper, we propose a smooth objective function to characterize the requirements of an acceptable winding-number field, which allows one to find the globally consistent normal orientations starting from a set of completely random normals. By taking the vertices of the Voronoi diagram of the point cloud as examination points, we consider the following three requirements: (1) the winding number is either 0 or 1, (2) the occurrences of 1 and the occurrences of 0 are balanced around the point cloud, and (3) the normals align with the outside Voronoi poles as much as possible. Extensive experimental results show that our method outperforms the existing approaches, especially in handling sparse and noisy point clouds, as well as shapes with complex geometry/topology.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Graphics},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\LXMPKBRP\\Xu 等 - 2023 - Globally Consistent Normal Orientation for Point C.pdf;C\:\\Users\\Color\\Zotero\\storage\\Q6DDNPJV\\2304.html}
}

@misc{xuGridguidedNeuralRadiance2023,
  title         = {Grid-Guided {{Neural Radiance Fields}} for {{Large Urban Scenes}}},
  author        = {Xu, Linning and Xiangli, Yuanbo and Peng, Sida and Pan, Xingang and Zhao, Nanxuan and Theobalt, Christian and Dai, Bo and Lin, Dahua},
  year          = {2023},
  month         = mar,
  number        = {arXiv:2303.14001},
  eprint        = {2303.14001},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2303.14001},
  urldate       = {2023-04-02},
  abstract      = {Purely MLP-based neural radiance fields (NeRF-based methods) often suffer from underfitting with blurred renderings on large-scale scenes due to limited model capacity. Recent approaches propose to geographically divide the scene and adopt multiple sub-NeRFs to model each region individually, leading to linear scale-up in training costs and the number of sub-NeRFs as the scene expands. An alternative solution is to use a feature grid representation, which is computationally efficient and can naturally scale to a large scene with increased grid resolutions. However, the feature grid tends to be less constrained and often reaches suboptimal solutions, producing noisy artifacts in renderings, especially in regions with complex geometry and texture. In this work, we present a new framework that realizes high-fidelity rendering on large urban scenes while being computationally efficient. We propose to use a compact multiresolution ground feature plane representation to coarsely capture the scene, and complement it with positional encoding inputs through another NeRF branch for rendering in a joint learning fashion. We show that such an integration can utilize the advantages of two alternative solutions: a light-weighted NeRF is sufficient, under the guidance of the feature grid representation, to render photorealistic novel views with fine details; and the jointly optimized ground feature planes, can meanwhile gain further refinements, forming a more accurate and compact feature space and output much more natural rendering results.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\BM6FCBHY\\Xu et al. - 2023 - Grid-guided Neural Radiance Fields for Large Urban.pdf;C\:\\Users\\Color\\Zotero\\storage\\E4GYXEWA\\2303.html}
}

@article{xuPointNeRFPointbasedNeurala,
  title  = {Point-{{NeRF}}: {{Point-based Neural Radiance Fields}}},
  author = {Xu, Qiangeng and Xu, Zexiang and Philip, Julien and Bi, Sai and Shu, Zhixin and Sunkavalli, Kalyan and Neumann, Ulrich},
  langid = {english},
  file   = {C\:\\Users\\Color\\Zotero\\storage\\WR2S28FX\\Xu 等 - Point-NeRF Point-based Neural Radiance Fields.pdf}
}

@misc{yangFreeNeRFImprovingFewshot2023,
  title         = {{{FreeNeRF}}: {{Improving Few-shot Neural Rendering}} with {{Free Frequency Regularization}}},
  shorttitle    = {{{FreeNeRF}}},
  author        = {Yang, Jiawei and Pavone, Marco and Wang, Yue},
  year          = {2023},
  month         = mar,
  number        = {arXiv:2303.07418},
  eprint        = {2303.07418},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2303.07418},
  urldate       = {2023-04-17},
  abstract      = {Novel view synthesis with sparse inputs is a challenging problem for neural radiance fields (NeRF). Recent efforts alleviate this challenge by introducing external supervision, such as pre-trained models and extra depth signals, and by non-trivial patch-based rendering. In this paper, we present Frequency regularized NeRF (FreeNeRF), a surprisingly simple baseline that outperforms previous methods with minimal modifications to the plain NeRF. We analyze the key challenges in few-shot neural rendering and find that frequency plays an important role in NeRF's training. Based on the analysis, we propose two regularization terms. One is to regularize the frequency range of NeRF's inputs, while the other is to penalize the near-camera density fields. Both techniques are ``free lunches'' at no additional computational cost. We demonstrate that even with one line of code change, the original NeRF can achieve similar performance as other complicated methods in the few-shot setting. FreeNeRF achieves state-of-the-art performance across diverse datasets, including Blender, DTU, and LLFF. We hope this simple baseline will motivate a rethinking of the fundamental role of frequency in NeRF's training under the low-data regime and beyond.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\WWFS6FYF\\Yang 等 - 2023 - FreeNeRF Improving Few-shot Neural Rendering with.pdf;C\:\\Users\\Color\\Zotero\\storage\\YFXBEMXE\\2303.html}
}

@misc{yangS3NeRFNeuralReflectance2022,
  title        = {S3-{{NeRF}}: {{Neural Reflectance Field}} from {{Shading}} and {{Shadow}} under a {{Single Viewpoint}}},
  shorttitle   = {S\$\^3\$-{{NeRF}}},
  author       = {Yang, Wenqi and Chen, Guanying and Chen, Chaofeng and Chen, Zhenfang and Wong, Kwan-Yee K.},
  year         = {2022},
  month        = oct,
  journal      = {arXiv.org},
  doi          = {10.48550/arXiv.2210.08936},
  urldate      = {2023-02-06},
  abstract     = {In this paper, we address the "dual problem" of multi-view scene reconstruction in which we utilize single-view images captured under different point lights to learn a neural scene representation. Different from existing single-view methods which can only recover a 2.5D scene representation (i.e., a normal / depth map for the visible surface), our method learns a neural reflectance field to represent the 3D geometry and BRDFs of a scene. Instead of relying on multi-view photo-consistency, our method exploits two information-rich monocular cues, namely shading and shadow, to infer scene geometry. Experiments on multiple challenging datasets show that our method is capable of recovering 3D geometry, including both visible and invisible parts, of a scene from single-view images. Thanks to the neural reflectance field representation, our method is robust to depth discontinuities. It supports applications like novel-view synthesis and relighting. Our code and model can be found at https://ywq.github.io/s3nerf.},
  howpublished = {https://arxiv.org/abs/2210.08936v1},
  langid       = {english},
  file         = {C\:\\Users\\Color\\Zotero\\storage\\699GINLH\\Yang 等 - 2022 - S$^3$-NeRF Neural Reflectance Field from Shading .pdf;D\:\\data\\papers\\terrain\\s3nerf.pdf}
}

@misc{yuanSurveyDevelopableSurfaces2023,
  title         = {A {{Survey}} of {{Developable Surfaces}}: {{From Shape Modeling}} to {{Manufacturing}}},
  shorttitle    = {A {{Survey}} of {{Developable Surfaces}}},
  author        = {Yuan, Chao and Cao, Nan and Shi, Yang},
  year          = {2023},
  month         = apr,
  number        = {arXiv:2304.09587},
  eprint        = {2304.09587},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2304.09587},
  urldate       = {2023-04-25},
  abstract      = {Developable surfaces are commonly observed in various applications such as architecture, product design, manufacturing, and mechanical materials, as well as in the development of tangible interaction and deformable robots, with the characteristics of easy-to-product, low-cost, transport-friendly, and deformable. Transforming shapes into developable surfaces is a complex and comprehensive task, which forms a variety of methods of segmentation, unfolding, and manufacturing for shapes with different geometry and topology, resulting in the complexity of developable surfaces. In this paper, we reviewed relevant methods and techniques for the study of developable surfaces, characterize them with our proposed pipeline, and categorize them based on digital modeling, physical modeling, interaction, and application. Through the analysis to the relevant literature, we also discussed some of the research challenges and future research opportunities.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Graphics},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\82TKZT2T\\Yuan 等 - 2023 - A Survey of Developable Surfaces From Shape Model.pdf;C\:\\Users\\Color\\Zotero\\storage\\PYB2BEHR\\2304.html}
}

@misc{yuMVImgNetLargescaleDataset2023,
  title         = {{{MVImgNet}}: {{A Large-scale Dataset}} of {{Multi-view Images}}},
  shorttitle    = {{{MVImgNet}}},
  author        = {Yu, Xianggang and Xu, Mutian and Zhang, Yidan and Liu, Haolin and Ye, Chongjie and Wu, Yushuang and Yan, Zizheng and Zhu, Chenming and Xiong, Zhangyang and Liang, Tianyou and Chen, Guanying and Cui, Shuguang and Han, Xiaoguang},
  year          = {2023},
  month         = mar,
  number        = {arXiv:2303.06042},
  eprint        = {2303.06042},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2303.06042},
  urldate       = {2023-04-17},
  abstract      = {Being data-driven is one of the most iconic properties of deep learning algorithms. The birth of ImageNet drives a remarkable trend of "learning from large-scale data" in computer vision. Pretraining on ImageNet to obtain rich universal representations has been manifested to benefit various 2D visual tasks, and becomes a standard in 2D vision. However, due to the laborious collection of real-world 3D data, there is yet no generic dataset serving as a counterpart of ImageNet in 3D vision, thus how such a dataset can impact the 3D community is unraveled. To remedy this defect, we introduce MVImgNet, a large-scale dataset of multi-view images, which is highly convenient to gain by shooting videos of real-world objects in human daily life. It contains 6.5 million frames from 219,188 videos crossing objects from 238 classes, with rich annotations of object masks, camera parameters, and point clouds. The multi-view attribute endows our dataset with 3D-aware signals, making it a soft bridge between 2D and 3D vision. We conduct pilot studies for probing the potential of MVImgNet on a variety of 3D and 2D visual tasks, including radiance field reconstruction, multi-view stereo, and view-consistent image understanding, where MVImgNet demonstrates promising performance, remaining lots of possibilities for future explorations. Besides, via dense reconstruction on MVImgNet, a 3D object point cloud dataset is derived, called MVPNet, covering 87,200 samples from 150 categories, with the class label on each point cloud. Experiments show that MVPNet can benefit the real-world 3D object classification while posing new challenges to point cloud understanding. MVImgNet and MVPNet will be publicly available, hoping to inspire the broader vision community.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\MMGTXV2Y\\Yu 等 - 2023 - MVImgNet A Large-scale Dataset of Multi-view Imag.pdf;C\:\\Users\\Color\\Zotero\\storage\\6SHRKVZ6\\2303.html}
}

@misc{yuPlenoxelsRadianceFields2021,
  title         = {Plenoxels: {{Radiance Fields}} without {{Neural Networks}}},
  shorttitle    = {Plenoxels},
  author        = {Yu, Alex and {Fridovich-Keil}, Sara and Tancik, Matthew and Chen, Qinhong and Recht, Benjamin and Kanazawa, Angjoo},
  year          = {2021},
  month         = dec,
  number        = {arXiv:2112.05131},
  eprint        = {2112.05131},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2112.05131},
  urldate       = {2023-04-20},
  abstract      = {We introduce Plenoxels (plenoptic voxels), a system for photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\KB7PPEW6\\Yu 等 - 2021 - Plenoxels Radiance Fields without Neural Networks.pdf;C\:\\Users\\Color\\Zotero\\storage\\D8QKIJGC\\2112.html}
}

@misc{zhouNeRFLiXHighQualityNeural2023,
  title         = {{{NeRFLiX}}: {{High-Quality Neural View Synthesis}} by {{Learning}} a {{Degradation-Driven Inter-viewpoint MiXer}}},
  shorttitle    = {{{NeRFLiX}}},
  author        = {Zhou, Kun and Li, Wenbo and Wang, Yi and Hu, Tao and Jiang, Nianjuan and Han, Xiaoguang and Lu, Jiangbo},
  year          = {2023},
  month         = mar,
  number        = {arXiv:2303.06919},
  eprint        = {2303.06919},
  primaryclass  = {cs},
  publisher     = {{arXiv}},
  doi           = {10.48550/arXiv.2303.06919},
  urldate       = {2023-04-17},
  abstract      = {Neural radiance fields (NeRF) show great success in novel view synthesis. However, in real-world scenes, recovering high-quality details from the source images is still challenging for the existing NeRF-based approaches, due to the potential imperfect calibration information and scene representation inaccuracy. Even with high-quality training frames, the synthetic novel views produced by NeRF models still suffer from notable rendering artifacts, such as noise, blur, etc. Towards to improve the synthesis quality of NeRF-based approaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by learning a degradation-driven inter-viewpoint mixer. Specially, we design a NeRF-style degradation modeling approach and construct large-scale training data, enabling the possibility of effectively removing NeRF-native rendering artifacts for existing deep neural networks. Moreover, beyond the degradation removal, we propose an inter-viewpoint aggregation framework that is able to fuse highly related high-quality training images, pushing the performance of cutting-edge NeRF models to entirely new levels and producing highly photo-realistic synthetic views.},
  archiveprefix = {arxiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {C\:\\Users\\Color\\Zotero\\storage\\RSF9MV3X\\Zhou 等 - 2023 - NeRFLiX High-Quality Neural View Synthesis by Lea.pdf;C\:\\Users\\Color\\Zotero\\storage\\Q3J5I24R\\2303.html}
}
